{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsloth Fine-tuning on Google Colab\n",
    "\n",
    "Train and fine-tune LLMs with Unsloth on Google Colab's free GPU.\n",
    "\n",
    "**Before you start:**\n",
    "1. Runtime â†’ Change runtime type â†’ GPU â†’ T4 GPU (free tier)\n",
    "2. Make a copy of this notebook to your Google Drive\n",
    "\n",
    "**Total time:** ~10-15 minutes (setup + training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "Install dependencies (takes ~5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install dependencies in the correct order\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Core ML frameworks\n",
    "!pip install \"trl>=0.12.0\" \"peft>=0.13.0\" \"bitsandbytes>=0.45.0\" \"transformers[sentencepiece]>=4.46.0\"\n",
    "\n",
    "# PyTorch\n",
    "!pip install torch==2.8.0 torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Unsloth\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# xformers\n",
    "!pip install --no-deps \"xformers>=0.0.32,<0.0.33\" --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Additional dependencies\n",
    "!pip install datasets huggingface_hub accelerate sentencepiece protobuf python-dotenv\n",
    "\n",
    "print(\"âœ… Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/farhan-syah/unsloth-finetuning.git\n",
    "%cd unsloth-finetuning\n",
    "\n",
    "print(\"âœ… Repository cloned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Training\n",
    "\n",
    "Edit these settings for your training run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ============================================\n# CONFIGURATION - Edit these settings\n# ============================================\n\n# Model Selection (choose based on use case)\nLORA_BASE_MODEL = \"unsloth/Qwen3-VL-2B-Instruct-unsloth-bnb-4bit\"  # 2B model, fits T4 GPU\n# LORA_BASE_MODEL = \"unsloth/Qwen3-4B-unsloth-bnb-4bit\"  # 4B model (needs A100)\n\n# Dataset\nDATASET_NAME = \"yahma/alpaca-cleaned\"  # Change to your dataset\n\n# Training Mode\n# Quick test (recommended for first run)\nMAX_STEPS = 50              # Train for 50 steps only (~2 minutes)\nDATASET_MAX_SAMPLES = 100   # Use 100 samples only\n\n# Full training (uncomment to use)\n# MAX_STEPS = 0               # Train for full epochs\n# DATASET_MAX_SAMPLES = 0     # Use all samples\n\n# Training Parameters\nMAX_SEQ_LENGTH = 2048\nLORA_RANK = 16              # Use 64 for production\nLORA_ALPHA = 32             # Use 128 for production\nBATCH_SIZE = 2\nGRADIENT_ACCUMULATION_STEPS = 2\nLEARNING_RATE = 2e-4\nNUM_TRAIN_EPOCHS = 1\nWARMUP_STEPS = 2\n\n# Output Formats (GGUF requires llama.cpp - not available in Colab)\nOUTPUT_FORMATS = \"\"  # Empty = no GGUF conversion (recommended for Colab)\n# OUTPUT_FORMATS = \"gguf_q4_k_m\"  # Only works with local setup\n\n# Output naming\nOUTPUT_MODEL_NAME = \"auto\"  # Auto-generate name\n\n# Author\nAUTHOR_NAME = \"Your Name\"  # Your name for model card\n\nprint(\"âœ… Configuration set!\")\nprint(f\"Model: {LORA_BASE_MODEL}\")\nprint(f\"Dataset: {DATASET_NAME}\")\nprint(f\"Training: {MAX_STEPS} steps, {DATASET_MAX_SAMPLES} samples\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create .env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create .env file with configuration\n",
    "env_content = f\"\"\"\n",
    "# Model\n",
    "LORA_BASE_MODEL={LORA_BASE_MODEL}\n",
    "INFERENCE_BASE_MODEL=\n",
    "OUTPUT_MODEL_NAME={OUTPUT_MODEL_NAME}\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME={DATASET_NAME}\n",
    "DATASET_MAX_SAMPLES={DATASET_MAX_SAMPLES}\n",
    "MAX_STEPS={MAX_STEPS}\n",
    "\n",
    "# Training\n",
    "MAX_SEQ_LENGTH={MAX_SEQ_LENGTH}\n",
    "LORA_RANK={LORA_RANK}\n",
    "LORA_ALPHA={LORA_ALPHA}\n",
    "BATCH_SIZE={BATCH_SIZE}\n",
    "GRADIENT_ACCUMULATION_STEPS={GRADIENT_ACCUMULATION_STEPS}\n",
    "LEARNING_RATE={LEARNING_RATE}\n",
    "NUM_TRAIN_EPOCHS={NUM_TRAIN_EPOCHS}\n",
    "WARMUP_STEPS={WARMUP_STEPS}\n",
    "PACKING=false\n",
    "\n",
    "# Optimization\n",
    "USE_GRADIENT_CHECKPOINTING=true\n",
    "MAX_GRAD_NORM=1.0\n",
    "OPTIM=adamw_8bit\n",
    "\n",
    "# Logging\n",
    "LOGGING_STEPS=5\n",
    "SAVE_STEPS=25\n",
    "SAVE_TOTAL_LIMIT=2\n",
    "SAVE_ONLY_FINAL=true\n",
    "\n",
    "# Monitoring\n",
    "WANDB_ENABLED=false\n",
    "\n",
    "# Output\n",
    "OUTPUT_FORMATS={OUTPUT_FORMATS}\n",
    "OUTPUT_DIR_BASE=./outputs\n",
    "PREPROCESSED_DATA_DIR=./data/preprocessed\n",
    "CACHE_DIR=./cache\n",
    "\n",
    "# HuggingFace\n",
    "PUSH_TO_HUB=false\n",
    "HF_USERNAME=your_username\n",
    "HF_MODEL_NAME=auto\n",
    "HF_TOKEN=\n",
    "\n",
    "# Author\n",
    "AUTHOR_NAME={AUTHOR_NAME}\n",
    "\n",
    "# Advanced\n",
    "SEED=3407\n",
    "FORCE_PREPROCESS=false\n",
    "FORCE_RETRAIN=true\n",
    "FORCE_REBUILD=true\n",
    "CHECK_SEQ_LENGTH=false\n",
    "\"\"\"\n",
    "\n",
    "with open('.env', 'w') as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(\"âœ… .env file created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train Model\n",
    "\n",
    "This will take ~2 minutes for quick test, or hours for full training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Build Merged Model\n\nThis creates the merged model (LoRA + base model combined) in safetensors format.\n\n**Why skip GGUF in Colab?**\n- GGUF conversion requires llama.cpp (not available in Colab)\n- **Better workflow:** Create merged model here, then convert to GGUF locally (CPU-only, no GPU needed)\n\n**This step creates:** `merged_16bit/` folder with complete model in safetensors format"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build merged model (safetensors format)\n# This skips GGUF since OUTPUT_FORMATS is empty\n!python build.py"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Save Your Model\n\n**You have two models to save:**\n\n1. **LoRA adapters** (~80-100MB) - Small, efficient, requires base model to use\n2. **Merged model** (~5GB for 2B) - Complete model, ready to use anywhere\n\n**Choose your preferred method:**\n- **Option A (Recommended):** HuggingFace Hub - Free, unlimited storage, easy sharing\n- **Option B:** Google Drive - Simple, but limited free storage (15GB)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check your model output\nimport os\n\n# List output directories\noutput_dirs = [d for d in os.listdir('outputs') if os.path.isdir(os.path.join('outputs', d))]\nif output_dirs:\n    model_dir = output_dirs[0]\n    print(f\"âœ… Your model is in: outputs/{model_dir}\")\n    print(f\"\\nContents:\")\n    !ls -lh outputs/{model_dir}\n    print(f\"\\nLoRA adapters: outputs/{model_dir}/lora/\")\n    print(f\"Merged model: outputs/{model_dir}/merged_16bit/\")\nelse:\n    print(\"âŒ No model found in outputs/\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### Option A: Push to HuggingFace Hub (Recommended)\n\n**Why HuggingFace?**\n- Free, unlimited storage\n- Easy sharing and version control\n- Direct integration with transformers, Ollama, etc.\n\n**Steps:**\n1. Get your HuggingFace token: https://huggingface.co/settings/tokens (create with \"Write\" access)\n2. Run the cells below to push both LoRA and merged models"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# A1. Configure HuggingFace settings\nfrom huggingface_hub import login, HfApi\nimport os\n\n# ============================================\n# EDIT THESE SETTINGS\n# ============================================\nHF_USERNAME = \"your-username\"  # Your HuggingFace username\nHF_TOKEN = \"\"  # Your HF token (or leave empty to enter interactively)\n\n# Repository names (auto-generated from model_dir by default)\n# Will create two repos:\n#   1. your-username/model-name-lora (LoRA adapters)\n#   2. your-username/model-name (Merged model)\nLORA_REPO_NAME = f\"{model_dir}-lora\"\nMERGED_REPO_NAME = f\"{model_dir}\"  # No suffix for merged model\n\n# ============================================\n\n# Update .env with HuggingFace username for cross-linking\nwith open('.env', 'a') as f:\n    f.write(f'\\nHF_USERNAME={HF_USERNAME}\\n')\n\nprint(f\"HuggingFace Username: {HF_USERNAME}\")\nprint(f\"\\nRepositories to create:\")\nprint(f\"   1. {HF_USERNAME}/{LORA_REPO_NAME} (LoRA adapters, ~80MB)\")\nprint(f\"   2. {HF_USERNAME}/{MERGED_REPO_NAME} (Merged model, ~5GB)\")\nprint(f\"\\nðŸ’¡ Later you can also create: {HF_USERNAME}/{model_dir}-gguf (for GGUF quantized)\")\nprint(f\"\\nReady to push? Run the next cell.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# A2. Push both models to HuggingFace Hub\nfrom huggingface_hub import login, HfApi, create_repo\nimport os\nimport subprocess\n\n# Login to HuggingFace\nif HF_TOKEN:\n    login(token=HF_TOKEN)\nelse:\n    login()  # Will prompt for token interactively\n\napi = HfApi()\n\n# Get model paths\nlora_path = f\"outputs/{model_dir}/lora\"\nmerged_path = f\"outputs/{model_dir}/merged_16bit\"\n\nprint(\"=\"*60)\nprint(\"UPLOADING TO HUGGINGFACE HUB\")\nprint(\"=\"*60)\n\n# Generate README files using standardized script\nprint(\"\\n[0/3] Generating model cards...\")\ntry:\n    result = subprocess.run(\n        [\"python\", \"generate_readme.py\"],\n        capture_output=True,\n        text=True,\n        timeout=10\n    )\n    if result.returncode == 0:\n        print(\"      âœ… Model cards generated\")\n    else:\n        print(f\"      âš ï¸  Warning: {result.stderr}\")\nexcept Exception as e:\n    print(f\"      âš ï¸  Could not generate model cards: {e}\")\n\n# 1. Push LoRA adapters\nlora_repo_id = f\"{HF_USERNAME}/{LORA_REPO_NAME}\"\nprint(f\"\\n[1/3] Pushing LoRA adapters to {lora_repo_id}...\")\nprint(f\"      Size: ~80-100MB\")\n\ntry:\n    create_repo(repo_id=lora_repo_id, repo_type=\"model\", exist_ok=True)\n    api.upload_folder(\n        folder_path=lora_path,\n        repo_id=lora_repo_id,\n        repo_type=\"model\",\n        commit_message=\"Upload LoRA adapters\"\n    )\n    print(f\"      âœ… LoRA adapters uploaded!\")\n    print(f\"      ðŸ”— https://huggingface.co/{lora_repo_id}\")\nexcept Exception as e:\n    print(f\"      âŒ Error: {e}\")\n\n# 2. Push merged model\nmerged_repo_id = f\"{HF_USERNAME}/{MERGED_REPO_NAME}\"\nprint(f\"\\n[2/3] Pushing merged model to {merged_repo_id}...\")\nprint(f\"      Size: ~5GB (this will take several minutes)\")\n\ntry:\n    create_repo(repo_id=merged_repo_id, repo_type=\"model\", exist_ok=True)\n    api.upload_folder(\n        folder_path=merged_path,\n        repo_id=merged_repo_id,\n        repo_type=\"model\",\n        commit_message=\"Upload merged model\"\n    )\n    print(f\"      âœ… Merged model uploaded!\")\n    print(f\"      ðŸ”— https://huggingface.co/{merged_repo_id}\")\nexcept Exception as e:\n    print(f\"      âŒ Error: {e}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"UPLOAD COMPLETE\")\nprint(\"=\"*60)\nprint(f\"\\nðŸ“¦ Your models on HuggingFace:\")\nprint(f\"   â€¢ LoRA: https://huggingface.co/{lora_repo_id}\")\nprint(f\"   â€¢ Merged: https://huggingface.co/{merged_repo_id}\")\nprint(f\"\\nðŸ’¡ Use the merged model with:\")\nprint(f\"   â€¢ transformers: model = AutoModelForCausalLM.from_pretrained('{merged_repo_id}')\")\nprint(f\"   â€¢ Ollama: ollama pull hf.co/{merged_repo_id}\")\nprint(f\"\\nðŸ“ Model cards generated by generate_readme.py (standardized)\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Option B: Google Drive (Alternative)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# B1. Upload to Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Copy to Google Drive\n!mkdir -p /content/drive/MyDrive/unsloth-models\n!cp -r outputs/* /content/drive/MyDrive/unsloth-models/\n\nprint(\"âœ… Model copied to Google Drive: MyDrive/unsloth-models/\")\nprint(\"\")\nprint(\"ðŸ“ Your model contains:\")\nprint(\"   - lora/ - LoRA adapters (~80MB)\")\nprint(\"   - merged_16bit/ - Merged model in safetensors format (~5GB for 2B model)\")\nprint(\"\")\nprint(\"âš ï¸  Note: Google Drive free tier has 15GB storage limit\")\nprint(\"Next: Download from Google Drive to convert to GGUF locally\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 8: Convert to GGUF Locally (Optional)\n\nAfter saving your model (HuggingFace or Google Drive), convert to GGUF on your local machine.\n\n**This works even with limited VRAM** - GGUF conversion is CPU-only, no GPU needed!\n\n### Option A: Download from HuggingFace and Convert\n\n```bash\n# 1. On your local machine, navigate to unsloth-finetuning\ncd /path/to/unsloth-finetuning\n\n# 2. Download merged model from HuggingFace\nhuggingface-cli download your-username/model-name-merged \\\n  --local-dir outputs/model-name-merged\n\n# 3. Update .env to enable GGUF conversion\n# Edit .env and set:\nOUTPUT_FORMATS=gguf_q4_k_m\n\n# 4. Run build.py (will only do GGUF conversion)\npython build.py\n```\n\n### Option B: Download from Google Drive and Convert\n\n```bash\n# 1. Download model from Google Drive to your local machine\n# Location: MyDrive/unsloth-models/Qwen3-VL-2B-Instruct-alpaca-cleaned/\n\n# 2. Navigate to your local unsloth-finetuning directory\ncd /path/to/unsloth-finetuning\n\n# 3. Copy merged model to outputs folder\ncp -r ~/Downloads/Qwen3-VL-2B-Instruct-alpaca-cleaned outputs/\n\n# 4. Update .env to enable GGUF conversion\n# Edit .env and set:\nOUTPUT_FORMATS=gguf_q4_k_m\n\n# 5. Run build.py (will only do GGUF conversion since merged model exists)\npython build.py\n```\n\n**Result:** You'll get `outputs/model-name/gguf/model.Q4_K_M.gguf`\n\n**Alternative - Direct llama.cpp conversion:**\n```bash\n# If you already ran setup.sh locally\n./llama.cpp/llama-quantize \\\n  outputs/model-name/merged_16bit/model.safetensors \\\n  outputs/model-name/gguf/model.Q4_K_M.gguf \\\n  q4_k_m\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 9: Test Your Model (Optional)\n\nQuick test of your fine-tuned model:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model_path = f\"outputs/{model_dir}/lora\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompt\n",
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "What is machine learning?\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, temperature=0.7)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(response)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Done!\n",
    "\n",
    "Your model has been trained and is ready to use!\n",
    "\n",
    "**Next steps:**\n",
    "1. Download the model from Google Drive or HuggingFace\n",
    "2. Use it locally with Ollama or transformers\n",
    "3. Share it on HuggingFace Hub\n",
    "\n",
    "**Resources:**\n",
    "- [Documentation](https://github.com/farhan-syah/unsloth-finetuning/tree/main/docs)\n",
    "- [Training Guide](https://github.com/farhan-syah/unsloth-finetuning/blob/main/docs/TRAINING.md)\n",
    "- [FAQ](https://github.com/farhan-syah/unsloth-finetuning/blob/main/docs/FAQ.md)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}