{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsloth Fine-tuning on Google Colab\n",
    "\n",
    "Train and fine-tune LLMs with Unsloth on Google Colab's free GPU.\n",
    "\n",
    "**Before you start:**\n",
    "1. Runtime \u2192 Change runtime type \u2192 GPU \u2192 T4 GPU (free tier)\n",
    "2. Make a copy of this notebook to your Google Drive\n",
    "\n",
    "**Total time:** varies by hardware (setup + training)"
   ],
   "id": "09a15606"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "Install dependencies (may take several minutes)"
   ],
   "id": "ab5a1805"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install dependencies in the correct order\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Core ML frameworks\n",
    "!pip install \"trl>=0.12.0\" \"peft>=0.13.0\" \"bitsandbytes>=0.45.0\" \"transformers[sentencepiece]>=4.46.0\"\n",
    "\n",
    "# PyTorch\n",
    "!pip install torch==2.8.0 torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Unsloth\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# xformers\n",
    "!pip install --no-deps \"xformers>=0.0.32,<0.0.33\" --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Additional dependencies\n",
    "!pip install datasets huggingface_hub accelerate sentencepiece protobuf python-dotenv\n",
    "\n",
    "print(\"\u2705 Installation complete!\")"
   ],
   "id": "b5e61d25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Repository"
   ],
   "id": "6b4ccb7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (removes old version if exists)\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Change to /content first (in case we're inside the repo)\n",
    "%cd /content\n",
    "\n",
    "repo_path = '/content/unsloth-finetuning'\n",
    "\n",
    "if os.path.exists(repo_path):\n",
    "    print(\"\ud83d\udd04 Removing old repository to fetch latest changes...\")\n",
    "    shutil.rmtree(repo_path)\n",
    "\n",
    "print(\"\ud83d\udce5 Cloning repository...\")\n",
    "!git clone https://github.com/farhan-syah/unsloth-finetuning.git\n",
    "%cd unsloth-finetuning\n",
    "print(\"\u2705 Repository cloned!\")\n",
    ""
   ],
   "id": "2b235375"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Configure Training\n\nCreate a custom YAML configuration or use the defaults.\n\n**Two options:**\n- **Quick test:** Use `quick_test.yaml` (already included, rank=32, 1 epoch, 100 samples)\n- **Custom config:** Create your own YAML below\n\nFor this demo, we'll create a custom Colab-optimized config.",
   "id": "ca47fe15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3322331"
   },
   "outputs": [],
   "source": "# Create custom YAML configuration for Colab\nyaml_config = \"\"\"\n# Colab-optimized configuration\n# Model selection\nmodel:\n  base_model: unsloth/Llama-3.2-1B-Instruct-bnb-4bit\n  inference_model: null  # Skip GGUF in Colab (no llama.cpp)\n  output_name: auto\n\n# Dataset\ndataset:\n  name: yahma/alpaca-cleaned\n  max_samples: 100  # Quick test - use 0 for full dataset\n\n# Training configuration\ntraining:\n  lora:\n    rank: 16\n    alpha: 32\n    dropout: 0.0\n    use_rslora: false\n  \n  batch:\n    size: 2\n    gradient_accumulation_steps: 4  # Effective batch size = 8\n  \n  optimization:\n    learning_rate: 2e-4\n    optimizer: adamw_8bit\n    warmup_ratio: 0.1\n    warmup_steps: 0\n    max_grad_norm: 1.0\n    use_gradient_checkpointing: true\n  \n  epochs:\n    num_train_epochs: 1\n    max_steps: 50  # Quick test - use 0 for full training\n  \n  data:\n    packing: false\n    seed: 3407\n    max_seq_length: 2048\n\n# Logging\nlogging:\n  logging_steps: 5\n  save_steps: 25\n  save_total_limit: 2\n  save_only_final: true\n\n# Output (skip GGUF for Colab)\noutput:\n  formats: []  # Empty = no GGUF conversion\n\n# Benchmark\nbenchmark:\n  max_tokens: 512\n  batch_size: 8\n  default_backend: transformers\n  default_tasks:\n    - ifeval\n\"\"\"\n\n# Save as colab_config.yaml\nwith open('colab_config.yaml', 'w') as f:\n    f.write(yaml_config)\n\nprint(\"\u2705 Created colab_config.yaml\")\nprint(\"\\n\ud83d\udcca Configuration:\")\nprint(\"   Model: Llama-3.2-1B-Instruct (4-bit)\")\nprint(\"   Dataset: alpaca-cleaned (100 samples)\")\nprint(\"   Training: 50 steps, LoRA rank 16\")\nprint(\"   Batch: 2 \u00d7 4 accumulation = 8 effective\")\nprint(\"   Output: Merged 16-bit only (no GGUF)\")\nprint(\"\\n\ud83d\udca1 Edit the yaml_config string above to customize settings\")",
   "id": "c28464dd"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 4: Preprocess Dataset\n\nAnalyze your dataset and get smart configuration recommendations.\n\n**This step:**\n- Preprocesses and analyzes your dataset (cached automatically)\n- Shows sequence length statistics\n- Recommends optimal batch size and training steps\n- No manual configuration needed - uses colab_config.yaml\n\n**Note:** Preprocessed data is cached. If you change the dataset or max_seq_length in Step 3, just rerun that cell - preprocessing will detect changes automatically.",
   "id": "00d70491",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Preprocess dataset using YAML config\n!python scripts/preprocess.py --config colab_config.yaml",
   "id": "520634d6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#",
    "#",
    " ",
    "S",
    "t",
    "e",
    "p",
    " ",
    "5",
    ":",
    " ",
    "T",
    "r",
    "a",
    "i",
    "n",
    " ",
    "M",
    "o",
    "d",
    "e",
    "l",
    "\n",
    "\n",
    "T",
    "r",
    "a",
    "i",
    "n",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "a",
    "u",
    "t",
    "o",
    "m",
    "a",
    "t",
    "i",
    "c",
    " ",
    "L",
    "o",
    "R",
    "A",
    " ",
    "b",
    "a",
    "c",
    "k",
    "u",
    "p",
    " ",
    "s",
    "y",
    "s",
    "t",
    "e",
    "m",
    " ",
    "-",
    " ",
    "e",
    "x",
    "i",
    "s",
    "t",
    "i",
    "n",
    "g",
    " ",
    "a",
    "d",
    "a",
    "p",
    "t",
    "e",
    "r",
    "s",
    " ",
    "a",
    "r",
    "e",
    " ",
    "b",
    "a",
    "c",
    "k",
    "e",
    "d",
    " ",
    "u",
    "p",
    " ",
    "a",
    "u",
    "t",
    "o",
    "m",
    "a",
    "t",
    "i",
    "c",
    "a",
    "l",
    "l",
    "y",
    ".",
    "\n",
    "\n",
    "*",
    "*",
    "T",
    "h",
    "i",
    "s",
    " ",
    "w",
    "i",
    "l",
    "l",
    " ",
    "t",
    "a",
    "k",
    "e",
    " ",
    "~",
    "2",
    " ",
    "m",
    "i",
    "n",
    "u",
    "t",
    "e",
    "s",
    " ",
    "f",
    "o",
    "r",
    " ",
    "q",
    "u",
    "i",
    "c",
    "k",
    " ",
    "t",
    "e",
    "s",
    "t",
    " ",
    "(",
    "5",
    "0",
    " ",
    "s",
    "t",
    "e",
    "p",
    "s",
    ")",
    "*",
    "*",
    "\n",
    "\n",
    "*",
    "*",
    "B",
    "a",
    "c",
    "k",
    "u",
    "p",
    " ",
    "s",
    "y",
    "s",
    "t",
    "e",
    "m",
    ":",
    "*",
    "*",
    "\n",
    "-",
    " ",
    "E",
    "x",
    "i",
    "s",
    "t",
    "i",
    "n",
    "g",
    " ",
    "L",
    "o",
    "R",
    "A",
    " ",
    "a",
    "d",
    "a",
    "p",
    "t",
    "e",
    "r",
    "s",
    " ",
    "a",
    "r",
    "e",
    " ",
    "a",
    "u",
    "t",
    "o",
    "m",
    "a",
    "t",
    "i",
    "c",
    "a",
    "l",
    "l",
    "y",
    " ",
    "b",
    "a",
    "c",
    "k",
    "e",
    "d",
    " ",
    "u",
    "p",
    " ",
    "b",
    "e",
    "f",
    "o",
    "r",
    "e",
    " ",
    "t",
    "r",
    "a",
    "i",
    "n",
    "i",
    "n",
    "g",
    "\n",
    "-",
    " ",
    "B",
    "a",
    "c",
    "k",
    "u",
    "p",
    "s",
    " ",
    "s",
    "a",
    "v",
    "e",
    "d",
    " ",
    "w",
    "i",
    "t",
    "h",
    " ",
    "t",
    "i",
    "m",
    "e",
    "s",
    "t",
    "a",
    "m",
    "p",
    "s",
    ":",
    " ",
    "`",
    "l",
    "o",
    "r",
    "a",
    ".",
    "b",
    "a",
    "c",
    "k",
    "u",
    "p",
    ".",
    "2",
    "0",
    "2",
    "5",
    "0",
    "1",
    "2",
    "4",
    "_",
    "1",
    "4",
    "3",
    "0",
    "2",
    "2",
    "/",
    "`",
    "\n",
    "-",
    " ",
    "N",
    "o",
    " ",
    "n",
    "e",
    "e",
    "d",
    " ",
    "f",
    "o",
    "r",
    " ",
    "m",
    "a",
    "n",
    "u",
    "a",
    "l",
    " ",
    "f",
    "i",
    "l",
    "e",
    " ",
    "m",
    "a",
    "n",
    "a",
    "g",
    "e",
    "m",
    "e",
    "n",
    "t"
   ],
   "id": "8a95688c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run training with YAML config\n!python scripts/train.py --config colab_config.yaml",
   "id": "6eb47813"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Build Merged Model\n\nCreate merged model (LoRA + base model combined) in safetensors format.\n\n**Why skip GGUF in Colab?**\n- GGUF conversion requires llama.cpp (not available in Colab)\n- **Better workflow:** Create merged model here, convert to GGUF locally (CPU-only, no GPU needed)\n\n**This step creates:** `merged_16bit/` folder with complete model",
   "id": "81997793"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build merged model using YAML config\n!python scripts/build.py --config colab_config.yaml",
   "id": "7def127d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Save Your Model\n\n**You have two models to save:**\n\n1. **LoRA adapters** (~80-100MB) - Small, efficient, requires base model to use\n2. **Merged model** (size varies by model) - Complete model, ready to use anywhere\n\n**Choose your preferred method:**\n- **Option A (Recommended):** HuggingFace Hub - Free, unlimited storage, easy sharing\n- **Option B:** Google Drive - Simple, but limited free storage (15GB)",
   "id": "cb0f808c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check your model output\nimport os\n\n# List output directories\noutput_dirs = [d for d in os.listdir('outputs') if os.path.isdir(os.path.join('outputs', d))]\nif output_dirs:\n    model_dir = output_dirs[0]\n    print(f\"\u2705 Your model is in: outputs/{model_dir}\")\n    print(f\"\\nContents:\")\n    !ls -lh outputs/{model_dir}\n    print(f\"\\nLoRA adapters: outputs/{model_dir}/lora/\")\n    print(f\"Merged model: outputs/{model_dir}/merged_16bit/\")\nelse:\n    print(\"\u274c No model found in outputs/\")",
   "id": "b3710aad"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Option A: Push to HuggingFace Hub (Recommended)\n\n**Why HuggingFace?**\n- Free, unlimited storage\n- Easy sharing and version control\n- Direct integration with transformers, Ollama, etc.\n\n**Steps:**\n1. Get your HuggingFace token: https://huggingface.co/settings/tokens (create with \"Write\" access)\n2. Run the cells below to push both LoRA and merged models",
   "id": "d6127822"
  },
  {
   "cell_type": "code",
   "source": "# A1. Configure HuggingFace settings\nfrom huggingface_hub import login, HfApi\nimport os\n\n# Try to get HF_TOKEN from Colab secrets first (recommended)\ntry:\n    from google.colab import userdata\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    print(\"\u2705 Using HF_TOKEN from Colab secrets\")\nexcept:\n    # Fallback to .env or interactive input\n    HF_TOKEN = os.getenv('HF_TOKEN', '')\n    if not HF_TOKEN:\n        print(\"\ud83d\udca1 TIP: Add HF_TOKEN to Colab secrets (\ud83d\udd11 icon in left sidebar) for easier reuse\")\n\n# If you didn't set HF_USERNAME in Step 3, set it here\nif not HF_USERNAME:\n    HF_USERNAME = \"your-username\"  # Your HuggingFace username\n\n# Repository names (auto-generated from model_dir by default)\nLORA_REPO_NAME = f\"{model_dir}-lora\"\nMERGED_REPO_NAME = f\"{model_dir}\"  # No suffix for merged model\n\nprint(f\"HuggingFace Username: {HF_USERNAME}\")\nprint(f\"\\nRepositories to create:\")\nprint(f\"   1. {HF_USERNAME}/{LORA_REPO_NAME} (LoRA adapters, ~80MB)\")\nprint(f\"   2. {HF_USERNAME}/{MERGED_REPO_NAME} (Merged model, size varies by model)\")\nprint(f\"\\n\ud83d\udca1 Later you can also create: {HF_USERNAME}/{model_dir}-GGUF (for GGUF quantized)\")\nprint(f\"\\n\ud83d\udcd6 How to set up Colab secrets:\")\nprint(f\"   1. Click the \ud83d\udd11 icon in the left sidebar\")\nprint(f\"   2. Add new secret: Name='HF_TOKEN', Value=<your token from https://huggingface.co/settings/tokens>\")\nprint(f\"   3. Toggle 'Notebook access' ON for this notebook\")\nprint(f\"\\nReady to push? Run the next cell.\")\n",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "1189e7ff"
  },
  {
   "cell_type": "code",
   "source": [
    "# A2. Push both models to HuggingFace Hub\n",
    "from huggingface_hub import login, HfApi, create_repo\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Login to HuggingFace\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "else:\n",
    "    print(\"\\n\ud83d\udd10 No HF_TOKEN found. Please enter your token:\")\n",
    "    print(\"   Get it from: https://huggingface.co/settings/tokens\")\n",
    "    login()  # Will prompt interactively\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Get model paths\n",
    "lora_path = f\"outputs/{model_dir}/lora\"\n",
    "merged_path = f\"outputs/{model_dir}/merged_16bit\"\n",
    "\n",
    "# Calculate sizes\n",
    "lora_size = sum(f.stat().st_size for f in Path(lora_path).rglob('*') if f.is_file())\n",
    "lora_size_mb = lora_size / (1024 * 1024)\n",
    "\n",
    "merged_size = sum(f.stat().st_size for f in Path(merged_path).rglob('*') if f.is_file())\n",
    "merged_size_gb = merged_size / (1024 * 1024 * 1024)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"UPLOADING TO HUGGINGFACE HUB\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate README files using standardized script\n",
    "print(\"\\n[0/3] Generating model cards...\")\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"python\", \"scripts/generate_readme_train.py\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=10\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"      \u2705 Model cards generated\")\n",
    "    else:\n",
    "        print(f\"      \u26a0\ufe0f  Warning: {result.stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"      \u26a0\ufe0f  Could not generate model cards: {e}\")\n",
    "\n",
    "# 1. Push LoRA adapters\n",
    "lora_repo_id = f\"{HF_USERNAME}/{LORA_REPO_NAME}\"\n",
    "print(f\"\\n[1/3] Pushing LoRA adapters to {lora_repo_id}...\")\n",
    "print(f\"      Size: {lora_size_mb:.1f} MB\")\n",
    "\n",
    "try:\n",
    "    create_repo(repo_id=lora_repo_id, repo_type=\"model\", exist_ok=True)\n",
    "    api.upload_folder(\n",
    "        folder_path=lora_path,\n",
    "        repo_id=lora_repo_id,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload LoRA adapters\"\n",
    "    )\n",
    "    print(f\"      \u2705 LoRA adapters uploaded!\")\n",
    "    print(f\"      \ud83d\udd17 https://huggingface.co/{lora_repo_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"      \u274c Error: {e}\")\n",
    "\n",
    "# 2. Push merged model\n",
    "merged_repo_id = f\"{HF_USERNAME}/{MERGED_REPO_NAME}\"\n",
    "print(f\"\\n[2/3] Pushing merged model to {merged_repo_id}...\")\n",
    "print(f\"      Size: {merged_size_gb:.2f} GB (this will take several minutes)\")\n",
    "\n",
    "try:\n",
    "    create_repo(repo_id=merged_repo_id, repo_type=\"model\", exist_ok=True)\n",
    "    api.upload_folder(\n",
    "        folder_path=merged_path,\n",
    "        repo_id=merged_repo_id,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload merged model\"\n",
    "    )\n",
    "    print(f\"      \u2705 Merged model uploaded!\")\n",
    "    print(f\"      \ud83d\udd17 https://huggingface.co/{merged_repo_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"      \u274c Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UPLOAD COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n\ud83d\udce6 Your models on HuggingFace:\")\n",
    "print(f\"   \u2022 LoRA: https://huggingface.co/{lora_repo_id}\")\n",
    "print(f\"   \u2022 Merged: https://huggingface.co/{merged_repo_id}\")\n",
    "print(f\"\\n\ud83d\udca1 Use the merged model with:\")\n",
    "print(f\"   \u2022 transformers: model = AutoModelForCausalLM.from_pretrained('{merged_repo_id}')\")\n",
    "print(f\"   \u2022 Ollama: ollama pull hf.co/{merged_repo_id}\")\n",
    "print(f\"\\n\ud83d\udcdd Model cards generated from training configuration\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "9ed8c544"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Option B: Google Drive (Alternative)",
   "id": "8c95761d"
  },
  {
   "cell_type": "code",
   "source": "# B1. Upload to Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Copy to Google Drive\n!mkdir -p /content/drive/MyDrive/unsloth-models\n!cp -r outputs/* /content/drive/MyDrive/unsloth-models/\n\nprint(\"\u2705 Model copied to Google Drive: MyDrive/unsloth-models/\")\nprint(\"\")\nprint(\"\ud83d\udcc1 Your model contains:\")\nprint(\"   - lora/ - LoRA adapters (~80MB)\")\nprint(\"   - merged_16bit/ - Merged model in safetensors format (size varies by model)\")\nprint(\"\")\nprint(\"\u26a0\ufe0f  Note: Google Drive free tier has 15GB storage limit\")\nprint(\"Next: Download from Google Drive to convert to GGUF locally\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "d4d761ee"
  },
  {
   "cell_type": "markdown",
   "source": "## Step 8: Convert to GGUF Locally (Optional)\n\nAfter uploading to HuggingFace, you can download and convert to GGUF on your local machine.\n\n**Why local conversion?**\n- GGUF conversion is CPU-only (no GPU needed, works on any machine)\n- llama.cpp not available in Colab\n- Better for creating multiple quantization formats\n\n---\n\n### Quick Start: Download and Convert in 3 Commands\n\nRun these commands on your **local machine**:\n\n```bash\n# 1. Setup (one-time only)\ngit clone https://github.com/farhan-syah/unsloth-finetuning.git\ncd unsloth-finetuning\nbash setup.sh  # Installs dependencies + llama.cpp\n\n# 2. Download your models from HuggingFace\npython scripts/push.py --all\n\n# 3. Create YAML config with your HF username\ncat > my_config.yaml <<EOF\nmodel:\n  base_model: unsloth/Llama-3.2-1B-Instruct-bnb-4bit\n  inference_model: unsloth/Llama-3.2-1B-Instruct\n  output_name: auto\n\ndataset:\n  name: yahma/alpaca-cleaned\n  max_samples: 0\n\noutput:\n  formats:\n    - gguf_q4_k_m\n    - gguf_q5_k_m\nEOF\n\n# 4. Convert to GGUF\npython scripts/build.py --config my_config.yaml\n```\n\nThat's it! Your GGUF files will be in `outputs/[model-name]/gguf/`\n\n---\n\n### Available GGUF Quantizations\n\nEdit the `output.formats` section in your YAML config:\n\n| Format | Size (1B) | Quality | Use Case |\n|--------|-----------|---------|----------|\n| `gguf_q4_k_m` | ~0.6GB | Good | **Recommended** - best balance |\n| `gguf_q5_k_m` | ~0.8GB | Better | Higher quality |\n| `gguf_q8_0` | ~1.2GB | Excellent | Near original quality |\n| `gguf_f16` | ~2.0GB | Best | Full precision (largest) |\n\nExample for multiple formats:\n```yaml\noutput:\n  formats:\n    - gguf_q4_k_m\n    - gguf_q5_k_m\n    - gguf_q8_0\n```\n\n---\n\n### Using Your GGUF Model\n\n#### With Ollama:\n\n```bash\ncd outputs/Llama-3.2-1B-alpaca-cleaned/gguf\nollama create my-model -f Modelfile\nollama run my-model \"Hello!\"\n```\n\n#### With llama.cpp:\n\n```bash\n./llama.cpp/llama-cli \\\n  -m outputs/model-name/gguf/model.Q4_K_M.gguf \\\n  -p \"Hello!\" --temp 0.7\n```",
   "metadata": {},
   "id": "2868669a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 9: Test Your Model (Optional)\n\nQuick test of your fine-tuned model:",
   "id": "6537b358"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model_path = f\"outputs/{model_dir}/lora\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompt\n",
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "What is machine learning?\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, temperature=0.7)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(response)\n",
    "print(\"=\"*50)"
   ],
   "id": "2512aae5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udf89 Done!\n",
    "\n",
    "Your model has been trained and is ready to use!\n",
    "\n",
    "**Next steps:**\n",
    "1. Download the model from Google Drive or HuggingFace\n",
    "2. Use it locally with Ollama or transformers\n",
    "3. Share it on HuggingFace Hub\n",
    "\n",
    "**Resources:**\n",
    "- [Documentation](https://github.com/farhan-syah/unsloth-finetuning/tree/main/docs)\n",
    "- [Training Guide](https://github.com/farhan-syah/unsloth-finetuning/blob/main/docs/TRAINING.md)\n",
    "- [FAQ](https://github.com/farhan-syah/unsloth-finetuning/blob/main/docs/FAQ.md)"
   ],
   "id": "fb3ac790"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}