{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsloth Fine-tuning on Google Colab\n",
    "\n",
    "Train and fine-tune LLMs with Unsloth on Google Colab's free GPU.\n",
    "\n",
    "**Before you start:**\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí T4 GPU (free tier)\n",
    "2. Make a copy of this notebook to your Google Drive\n",
    "\n",
    "**Total time:** ~10-15 minutes (setup + training)"
   ],
   "id": "09a15606"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment\n",
    "\n",
    "Install dependencies (takes ~5 minutes)"
   ],
   "id": "ab5a1805"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install dependencies in the correct order\n",
    "!pip install --upgrade pip\n",
    "\n",
    "# Core ML frameworks\n",
    "!pip install \"trl>=0.12.0\" \"peft>=0.13.0\" \"bitsandbytes>=0.45.0\" \"transformers[sentencepiece]>=4.46.0\"\n",
    "\n",
    "# PyTorch\n",
    "!pip install torch==2.8.0 torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Unsloth\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "# xformers\n",
    "!pip install --no-deps \"xformers>=0.0.32,<0.0.33\" --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Additional dependencies\n",
    "!pip install datasets huggingface_hub accelerate sentencepiece protobuf python-dotenv\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ],
   "id": "b5e61d25"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Repository"
   ],
   "id": "6b4ccb7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/farhan-syah/unsloth-finetuning.git\n",
    "%cd unsloth-finetuning\n",
    "\n",
    "print(\"‚úÖ Repository cloned!\")"
   ],
   "id": "2b235375"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Training\n",
    "\n",
    "Edit these settings for your training run:"
   ],
   "id": "ca47fe15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3322331"
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - Edit these settings\n",
    "# ============================================\n",
    "\n",
    "# Model Selection (choose based on use case)\n",
    "LORA_BASE_MODEL = \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\"  # 1.7B model, fits T4 GPU\n",
    "# LORA_BASE_MODEL = \"unsloth/Qwen3-VL-2B-Instruct-unsloth-bnb-4bit\"  # 2B model with vision\n",
    "# LORA_BASE_MODEL = \"unsloth/Qwen3-4B-unsloth-bnb-4bit\"  # 4B model (needs A100)\n",
    "\n",
    "# Inference/Merging Model (OPTIONAL - for higher quality merged model)\n",
    "# Leave empty to use LORA_BASE_MODEL (faster, uses 4-bit)\n",
    "# Uncomment to use unquantized model for true 16-bit quality (slower, requires more VRAM)\n",
    "INFERENCE_BASE_MODEL = \"\"  # Empty = use LORA_BASE_MODEL (4-bit, faster)\n",
    "# INFERENCE_BASE_MODEL = \"unsloth/Qwen3-1.7B\"  # True 16-bit (requires ~15GB VRAM during build)\n",
    "# INFERENCE_BASE_MODEL = \"unsloth/Qwen3-VL-2B-Instruct\"  # For vision models\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = \"yahma/alpaca-cleaned\"  # Change to your dataset\n",
    "\n",
    "# Training Mode\n",
    "# Quick test (recommended for first run)\n",
    "MAX_STEPS = 50              # Train for 50 steps only (~2 minutes)\n",
    "DATASET_MAX_SAMPLES = 100   # Use 100 samples only\n",
    "\n",
    "# Full training (uncomment to use)\n",
    "# MAX_STEPS = 0               # Train for full epochs\n",
    "# DATASET_MAX_SAMPLES = 0     # Use all samples\n",
    "\n",
    "# ============================================\n",
    "# HUGGINGFACE CONFIGURATION (Optional)\n",
    "# ============================================\n",
    "# Set your HuggingFace username here if you plan to push to HF Hub in Step 8\n",
    "# This allows proper model card generation with cross-links between repos\n",
    "HF_USERNAME = \"\"  # Your HuggingFace username (e.g., \"your-username\")\n",
    "\n",
    "# ============================================\n",
    "# TRAINING HYPERPARAMETERS\n",
    "# ============================================\n",
    "\n",
    "# Sequence Length\n",
    "# Modern models support 8k-32k+ but longer = quadratic VRAM usage\n",
    "# 512-1024: Short instructions, 2048-4096: Balanced, 8192+: Long context\n",
    "MAX_SEQ_LENGTH = 4096       # Maximum tokens per training sample (reduce if OOM)\n",
    "\n",
    "# LoRA Configuration\n",
    "# Rank: Controls trainable parameters (8, 16, 32, 64, 128)\n",
    "# Alpha: Scaling factor (typically r or r*2)\n",
    "LORA_RANK = 16              # Recommended: 16 or 32\n",
    "LORA_ALPHA = 32             # Recommended: same as rank or 2x rank\n",
    "\n",
    "# Batch Size Configuration\n",
    "# EFFECTIVE BATCH SIZE = BATCH_SIZE √ó GRADIENT_ACCUMULATION_STEPS\n",
    "# Target: 8-16 for stable training\n",
    "BATCH_SIZE = 2              # Samples per GPU pass (reduce to 1 if OOM)\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Micro-batches before update\n",
    "# Current Effective Batch Size: 2 √ó 4 = 8\n",
    "\n",
    "# For faster training with T4 (15GB VRAM):\n",
    "# BATCH_SIZE = 4, GRADIENT_ACCUMULATION_STEPS = 2  ‚Üí Effective = 8 (faster)\n",
    "# For maximum speed (if no OOM):\n",
    "# BATCH_SIZE = 8, GRADIENT_ACCUMULATION_STEPS = 2  ‚Üí Effective = 16 (fastest)\n",
    "\n",
    "# Learning Rate & Schedule\n",
    "LEARNING_RATE = 2e-4        # Standard for LoRA (2e-4 recommended)\n",
    "NUM_TRAIN_EPOCHS = 1        # Number of passes through dataset\n",
    "WARMUP_STEPS = 5            # Gradual LR ramp-up (5-10% of total steps)\n",
    "\n",
    "# Optimization Settings\n",
    "PACKING = False             # Pack short sequences (True = faster for short texts)\n",
    "USE_GRADIENT_CHECKPOINTING = True  # False = faster but more VRAM\n",
    "\n",
    "# For SPEED OPTIMIZATION on T4 (if no OOM):\n",
    "# USE_GRADIENT_CHECKPOINTING = False\n",
    "# BATCH_SIZE = 4 or 8\n",
    "# PACKING = True\n",
    "\n",
    "# Output Formats (GGUF requires llama.cpp - not available in Colab)\n",
    "OUTPUT_FORMATS = \"\"  # Empty = no GGUF conversion (recommended for Colab)\n",
    "\n",
    "# Output naming\n",
    "OUTPUT_MODEL_NAME = \"auto\"  # Auto-generate name\n",
    "\n",
    "# Author\n",
    "AUTHOR_NAME = \"Your Name\"  # Your name for model card\n",
    "\n",
    "# ============================================\n",
    "# CALCULATED VALUES\n",
    "# ============================================\n",
    "effective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "print(\"‚úÖ Configuration set!\")\n",
    "print(f\"\\nüìä Model Configuration:\")\n",
    "print(f\"   Training Model: {LORA_BASE_MODEL}\")\n",
    "print(f\"   Merging Model: {INFERENCE_BASE_MODEL if INFERENCE_BASE_MODEL else 'Same as training (4-bit)'}\")\n",
    "print(f\"\\nüìö Dataset:\")\n",
    "print(f\"   Dataset: {DATASET_NAME}\")\n",
    "print(f\"   Training: {MAX_STEPS if MAX_STEPS > 0 else 'Full epochs'} steps, {DATASET_MAX_SAMPLES if DATASET_MAX_SAMPLES > 0 else 'All'} samples\")\n",
    "print(f\"\\n‚öôÔ∏è  Hyperparameters:\")\n",
    "print(f\"   Max Seq Length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"   LoRA Rank: {LORA_RANK}, Alpha: {LORA_ALPHA}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"   Gradient Accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"   Effective Batch Size: {effective_batch_size} {'‚úì Good' if 8 <= effective_batch_size <= 16 else '‚ö†Ô∏è  Consider 8-16'}\")\n",
    "print(f\"   Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"\\nüöÄ Optimization:\")\n",
    "print(f\"   Gradient Checkpointing: {'ON (slower, less VRAM)' if USE_GRADIENT_CHECKPOINTING else 'OFF (faster, more VRAM)'}\")\n",
    "print(f\"   Packing: {'ON' if PACKING else 'OFF'}\")\n",
    "if HF_USERNAME:\n",
    "    print(f\"\\nüì§ HuggingFace:\")\n",
    "    print(f\"   Username: {HF_USERNAME} (model cards will include HF links)\")"
   ],
   "id": "c28464dd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create .env File"
   ],
   "id": "4101c577"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6f012432"
   },
   "outputs": [],
   "source": [
    "# Create .env file with configuration\n",
    "env_content = f\"\"\"\n",
    "# Model\n",
    "LORA_BASE_MODEL={LORA_BASE_MODEL}\n",
    "INFERENCE_BASE_MODEL={INFERENCE_BASE_MODEL}\n",
    "OUTPUT_MODEL_NAME={OUTPUT_MODEL_NAME}\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME={DATASET_NAME}\n",
    "DATASET_MAX_SAMPLES={DATASET_MAX_SAMPLES}\n",
    "MAX_STEPS={MAX_STEPS}\n",
    "\n",
    "# Training\n",
    "MAX_SEQ_LENGTH={MAX_SEQ_LENGTH}\n",
    "LORA_RANK={LORA_RANK}\n",
    "LORA_ALPHA={LORA_ALPHA}\n",
    "BATCH_SIZE={BATCH_SIZE}\n",
    "GRADIENT_ACCUMULATION_STEPS={GRADIENT_ACCUMULATION_STEPS}\n",
    "LEARNING_RATE={LEARNING_RATE}\n",
    "NUM_TRAIN_EPOCHS={NUM_TRAIN_EPOCHS}\n",
    "WARMUP_STEPS={WARMUP_STEPS}\n",
    "PACKING={'true' if PACKING else 'false'}\n",
    "\n",
    "# Optimization\n",
    "USE_GRADIENT_CHECKPOINTING={'true' if USE_GRADIENT_CHECKPOINTING else 'false'}\n",
    "MAX_GRAD_NORM=1.0\n",
    "OPTIM=adamw_8bit\n",
    "\n",
    "# Logging\n",
    "LOGGING_STEPS=5\n",
    "SAVE_STEPS=25\n",
    "SAVE_TOTAL_LIMIT=2\n",
    "SAVE_ONLY_FINAL=true\n",
    "\n",
    "# Monitoring\n",
    "WANDB_ENABLED=false\n",
    "\n",
    "# Output\n",
    "OUTPUT_FORMATS={OUTPUT_FORMATS}\n",
    "OUTPUT_DIR_BASE=./outputs\n",
    "PREPROCESSED_DATA_DIR=./data/preprocessed\n",
    "CHECK_SEQ_LENGTH=true\n",
    "CACHE_DIR=./cache\n",
    "\n",
    "# HuggingFace\n",
    "PUSH_TO_HUB=false\n",
    "HF_USERNAME={HF_USERNAME}\n",
    "HF_MODEL_NAME=auto\n",
    "HF_TOKEN={HF_TOKEN}\n",
    "\n",
    "# Author\n",
    "AUTHOR_NAME={AUTHOR_NAME}\n",
    "\n",
    "# Advanced\n",
    "SEED=3407\n",
    "FORCE_PREPROCESS=false\n",
    "FORCE_RETRAIN=true\n",
    "FORCE_REBUILD=true\n",
    "CHECK_SEQ_LENGTH=false\n",
    "\"\"\"\n",
    "\n",
    "with open('.env', 'w') as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(\"‚úÖ .env file created!\")\n",
    "print(f\"\\n‚öôÔ∏è  Effective Batch Size: {effective_batch_size}\")\n",
    "if INFERENCE_BASE_MODEL:\n",
    "    print(f\"‚ö†Ô∏è  Using true 16-bit model for merging: {INFERENCE_BASE_MODEL}\")\n",
    "    print(f\"   This will require more VRAM during Step 6 (build)\")\n",
    "if not USE_GRADIENT_CHECKPOINTING:\n",
    "    print(f\"üöÄ Gradient checkpointing disabled for faster training\")\n",
    "if HF_USERNAME:\n",
    "    print(f\"üì§ HuggingFace username set: {HF_USERNAME}\")"
   ],
   "id": "f49d3232"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Preprocess Dataset\n",
    "\n",
    "Analyze your dataset and get smart configuration recommendations.\n",
    "\n",
    "**This step:**\n",
    "- Preprocesses and analyzes your dataset (cached, won't rerun if already done)\n",
    "- Shows sequence length statistics\n",
    "- Recommends optimal BATCH_SIZE, MAX_STEPS for 1-3 epochs\n",
    "- Analyzes GPU memory and suggests settings\n",
    "\n",
    "**After running this step:**\n",
    "1. Review the recommendations shown below\n",
    "2. If you want to use the recommended settings:\n",
    "   - Go back to Step 3 and update the configuration values\n",
    "   - Rerun Step 4 (Create .env File) to update the .env file\n",
    "   - Skip rerunning this step (preprocessed data is already saved)\n",
    "3. Continue to Step 6 (Train Model)\n",
    "\n",
    "**Note:** Preprocessed data is cached. If you change DATASET_NAME or MAX_SEQ_LENGTH, set `FORCE_PREPROCESS=true` in Step 3 before rerunning."
   ],
   "id": "00d70491",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset and get recommendations\n",
    "!python preprocess.py"
   ],
   "id": "520634d6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train Model\n",
    "\n",
    "This will take ~2 minutes for quick test, or hours for full training."
   ],
   "id": "8a95688c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "!python train.py"
   ],
   "id": "6eb47813"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Build Merged Model\n",
    "\n",
    "This creates the merged model (LoRA + base model combined) in safetensors format.\n",
    "\n",
    "**Why skip GGUF in Colab?**\n",
    "- GGUF conversion requires llama.cpp (not available in Colab)\n",
    "- **Better workflow:** Create merged model here, then convert to GGUF locally (CPU-only, no GPU needed)\n",
    "\n",
    "**This step creates:** `merged_16bit/` folder with complete model in safetensors format"
   ],
   "id": "81997793"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build merged model (safetensors format)\n# This skips GGUF since OUTPUT_FORMATS is empty\n!python build.py",
   "id": "7def127d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Your Model\n",
    "\n",
    "**You have two models to save:**\n",
    "\n",
    "1. **LoRA adapters** (~80-100MB) - Small, efficient, requires base model to use\n",
    "2. **Merged model** (size varies by model) - Complete model, ready to use anywhere\n",
    "\n",
    "**Choose your preferred method:**\n",
    "- **Option A (Recommended):** HuggingFace Hub - Free, unlimited storage, easy sharing\n",
    "- **Option B:** Google Drive - Simple, but limited free storage (15GB)"
   ],
   "id": "cb0f808c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check your model output\nimport os\n\n# List output directories\noutput_dirs = [d for d in os.listdir('outputs') if os.path.isdir(os.path.join('outputs', d))]\nif output_dirs:\n    model_dir = output_dirs[0]\n    print(f\"‚úÖ Your model is in: outputs/{model_dir}\")\n    print(f\"\\nContents:\")\n    !ls -lh outputs/{model_dir}\n    print(f\"\\nLoRA adapters: outputs/{model_dir}/lora/\")\n    print(f\"Merged model: outputs/{model_dir}/merged_16bit/\")\nelse:\n    print(\"‚ùå No model found in outputs/\")",
   "id": "b3710aad"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Option A: Push to HuggingFace Hub (Recommended)\n\n**Why HuggingFace?**\n- Free, unlimited storage\n- Easy sharing and version control\n- Direct integration with transformers, Ollama, etc.\n\n**Steps:**\n1. Get your HuggingFace token: https://huggingface.co/settings/tokens (create with \"Write\" access)\n2. Run the cells below to push both LoRA and merged models",
   "id": "d6127822"
  },
  {
   "cell_type": "code",
   "source": [
    "# A1. Configure HuggingFace settings\n",
    "from huggingface_hub import login, HfApi\n",
    "import os\n",
    "\n",
    "# Try to get HF_TOKEN from Colab secrets first (recommended)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"‚úÖ Using HF_TOKEN from Colab secrets\")\n",
    "except:\n",
    "    # Fallback to .env or interactive input\n",
    "    HF_TOKEN = os.getenv('HF_TOKEN', '')\n",
    "    if not HF_TOKEN:\n",
    "        print(\"üí° TIP: Add HF_TOKEN to Colab secrets (üîë icon in left sidebar) for easier reuse\")\n",
    "\n",
    "# If you didn't set HF_USERNAME in Step 3, set it here\n",
    "if not HF_USERNAME:\n",
    "    HF_USERNAME = \"your-username\"  # Your HuggingFace username\n",
    "\n",
    "# Repository names (auto-generated from model_dir by default)\n",
    "LORA_REPO_NAME = f\"{model_dir}-lora\"\n",
    "MERGED_REPO_NAME = f\"{model_dir}\"  # No suffix for merged model\n",
    "\n",
    "print(f\"HuggingFace Username: {HF_USERNAME}\")\n",
    "print(f\"\\nRepositories to create:\")\n",
    "print(f\"   1. {HF_USERNAME}/{LORA_REPO_NAME} (LoRA adapters, ~80MB)\")\n",
    "print(f\"   2. {HF_USERNAME}/{MERGED_REPO_NAME} (Merged model, size varies by model)\")\n",
    "print(f\"\\nüí° Later you can also create: {HF_USERNAME}/{model_dir}-gguf (for GGUF quantized)\")\n",
    "print(f\"\\nüìñ How to set up Colab secrets:\")\n",
    "print(f\"   1. Click the üîë icon in the left sidebar\")\n",
    "print(f\"   2. Add new secret: Name='HF_TOKEN', Value=<your token from https://huggingface.co/settings/tokens>\")\n",
    "print(f\"   3. Toggle 'Notebook access' ON for this notebook\")\n",
    "print(f\"\\nReady to push? Run the next cell.\")\n",
    ""
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "1189e7ff"
  },
  {
   "cell_type": "code",
   "source": [
    "# A2. Push both models to HuggingFace Hub\n",
    "from huggingface_hub import login, HfApi, create_repo\n",
    "from pathlib import Path\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Login to HuggingFace\n",
    "# Login to HuggingFace\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "else:\n",
    "else:\n",
    "    print(\"\\nüîê No HF_TOKEN found. Please enter your token:\")\n",
    "    print(\"   Get it from: https://huggingface.co/settings/tokens\")\n",
    "    login()  # Will prompt interactively\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# Get model paths\n",
    "lora_path = f\"outputs/{model_dir}/lora\"\n",
    "merged_path = f\"outputs/{model_dir}/merged_16bit\"\n",
    "\n",
    "# Calculate sizes\n",
    "lora_size = sum(f.stat().st_size for f in Path(lora_path).rglob('*') if f.is_file())\n",
    "lora_size_mb = lora_size / (1024 * 1024)\n",
    "\n",
    "merged_size = sum(f.stat().st_size for f in Path(merged_path).rglob('*') if f.is_file())\n",
    "merged_size_gb = merged_size / (1024 * 1024 * 1024)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"UPLOADING TO HUGGINGFACE HUB\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate README files using standardized script\n",
    "print(\"\\n[0/3] Generating model cards...\")\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"python\", \"generate_readme_train.py\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=10\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"      ‚úÖ Model cards generated\")\n",
    "    else:\n",
    "        print(f\"      ‚ö†Ô∏è  Warning: {result.stderr}\")\n",
    "except Exception as e:\n",
    "    print(f\"      ‚ö†Ô∏è  Could not generate model cards: {e}\")\n",
    "\n",
    "# 1. Push LoRA adapters\n",
    "lora_repo_id = f\"{HF_USERNAME}/{LORA_REPO_NAME}\"\n",
    "print(f\"\\n[1/3] Pushing LoRA adapters to {lora_repo_id}...\")\n",
    "print(f\"      Size: {lora_size_mb:.1f} MB\")\n",
    "\n",
    "try:\n",
    "    create_repo(repo_id=lora_repo_id, repo_type=\"model\", exist_ok=True)\n",
    "    api.upload_folder(\n",
    "        folder_path=lora_path,\n",
    "        repo_id=lora_repo_id,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload LoRA adapters\"\n",
    "    )\n",
    "    print(f\"      ‚úÖ LoRA adapters uploaded!\")\n",
    "    print(f\"      üîó https://huggingface.co/{lora_repo_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"      ‚ùå Error: {e}\")\n",
    "\n",
    "# 2. Push merged model\n",
    "merged_repo_id = f\"{HF_USERNAME}/{MERGED_REPO_NAME}\"\n",
    "print(f\"\\n[2/3] Pushing merged model to {merged_repo_id}...\")\n",
    "print(f\"      Size: {merged_size_gb:.2f} GB (this will take several minutes)\")\n",
    "\n",
    "try:\n",
    "    create_repo(repo_id=merged_repo_id, repo_type=\"model\", exist_ok=True)\n",
    "    api.upload_folder(\n",
    "        folder_path=merged_path,\n",
    "        repo_id=merged_repo_id,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload merged model\"\n",
    "    )\n",
    "    print(f\"      ‚úÖ Merged model uploaded!\")\n",
    "    print(f\"      üîó https://huggingface.co/{merged_repo_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"      ‚ùå Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"UPLOAD COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüì¶ Your models on HuggingFace:\")\n",
    "print(f\"   ‚Ä¢ LoRA: https://huggingface.co/{lora_repo_id}\")\n",
    "print(f\"   ‚Ä¢ Merged: https://huggingface.co/{merged_repo_id}\")\n",
    "print(f\"\\nüí° Use the merged model with:\")\n",
    "print(f\"   ‚Ä¢ transformers: model = AutoModelForCausalLM.from_pretrained('{merged_repo_id}')\")\n",
    "print(f\"   ‚Ä¢ Ollama: ollama pull hf.co/{merged_repo_id}\")\n",
    "print(f\"\\nüìù Model cards generated from training configuration\")"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "9ed8c544"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Option B: Google Drive (Alternative)",
   "id": "8c95761d"
  },
  {
   "cell_type": "code",
   "source": "# B1. Upload to Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Copy to Google Drive\n!mkdir -p /content/drive/MyDrive/unsloth-models\n!cp -r outputs/* /content/drive/MyDrive/unsloth-models/\n\nprint(\"‚úÖ Model copied to Google Drive: MyDrive/unsloth-models/\")\nprint(\"\")\nprint(\"üìÅ Your model contains:\")\nprint(\"   - lora/ - LoRA adapters (~80MB)\")\nprint(\"   - merged_16bit/ - Merged model in safetensors format (size varies by model)\")\nprint(\"\")\nprint(\"‚ö†Ô∏è  Note: Google Drive free tier has 15GB storage limit\")\nprint(\"Next: Download from Google Drive to convert to GGUF locally\")",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "id": "d4d761ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 9: Convert to GGUF Locally (Optional)\n",
    "\n",
    "After uploading to HuggingFace, you can download and convert to GGUF on your local machine.\n",
    "\n",
    "**Why local conversion?**\n",
    "- GGUF conversion is CPU-only (no GPU needed, works on any machine)\n",
    "- llama.cpp not available in Colab\n",
    "- Better for creating multiple quantization formats\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step: Download from HuggingFace and Convert to GGUF\n",
    "\n",
    "Run these commands on your **local machine**:\n",
    "\n",
    "```bash\n",
    "# ============================================\n",
    "# 1. Setup local environment (one-time)\n",
    "# ============================================\n",
    "# If you haven't set up locally yet:\n",
    "git clone https://github.com/farhan-syah/unsloth-finetuning.git\n",
    "cd unsloth-finetuning\n",
    "bash setup.sh  # Installs dependencies + llama.cpp\n",
    "\n",
    "# ============================================\n",
    "# 2. Download models from HuggingFace\n",
    "# ============================================\n",
    "# Download LoRA adapters\n",
    "hf download {HF_USERNAME}/{model_dir}-lora \\\n",
    "  --local-dir outputs/{model_dir}/lora\n",
    "\n",
    "# Download merged model\n",
    "hf download {HF_USERNAME}/{model_dir} \\\n",
    "  --local-dir outputs/{model_dir}/merged_16bit\n",
    "\n",
    "# ============================================\n",
    "# 3. Create .env file with your configuration\n",
    "# ============================================\n",
    "cat > .env <<'EOF'\n",
    "# Model (must match what was used in training)\n",
    "LORA_BASE_MODEL={LORA_BASE_MODEL}\n",
    "INFERENCE_BASE_MODEL={INFERENCE_BASE_MODEL if INFERENCE_BASE_MODEL else LORA_BASE_MODEL}\n",
    "OUTPUT_MODEL_NAME={OUTPUT_MODEL_NAME}\n",
    "\n",
    "# Dataset (for README generation)\n",
    "DATASET_NAME={DATASET_NAME}\n",
    "DATASET_MAX_SAMPLES={DATASET_MAX_SAMPLES}\n",
    "MAX_STEPS={MAX_STEPS}\n",
    "\n",
    "# Training params (for README generation)\n",
    "MAX_SEQ_LENGTH={MAX_SEQ_LENGTH}\n",
    "LORA_RANK={LORA_RANK}\n",
    "LORA_ALPHA={LORA_ALPHA}\n",
    "BATCH_SIZE={BATCH_SIZE}\n",
    "GRADIENT_ACCUMULATION_STEPS={GRADIENT_ACCUMULATION_STEPS}\n",
    "LEARNING_RATE={LEARNING_RATE}\n",
    "NUM_TRAIN_EPOCHS={NUM_TRAIN_EPOCHS}\n",
    "\n",
    "# Output formats - ADD YOUR DESIRED GGUF FORMATS HERE\n",
    "OUTPUT_FORMATS=gguf_q4_k_m,gguf_q5_k_m\n",
    "\n",
    "# HuggingFace (for model card generation)\n",
    "HF_USERNAME={HF_USERNAME if HF_USERNAME else \"your-username\"}\n",
    "HF_MODEL_NAME=auto\n",
    "\n",
    "# Author\n",
    "AUTHOR_NAME={AUTHOR_NAME}\n",
    "\n",
    "# Advanced\n",
    "OUTPUT_DIR_BASE=./outputs\n",
    "FORCE_REBUILD=false\n",
    "SEED=3407\n",
    "EOF\n",
    "\n",
    "# ============================================\n",
    "# 4. Convert to GGUF\n",
    "# ============================================\n",
    "# This will:\n",
    "# - Load merged_16bit model\n",
    "# - Create GGUF quantizations (Q4_K_M, Q5_K_M)\n",
    "# - Generate README for GGUF\n",
    "python build.py\n",
    "\n",
    "# ============================================\n",
    "# 5. Your GGUF files are ready!\n",
    "# ============================================\n",
    "# Location: outputs/{model_dir}/gguf/\n",
    "ls -lh outputs/{model_dir}/gguf/\n",
    "\n",
    "# Files created:\n",
    "# - model.Q4_K_M.gguf  (~1.0GB for 1.7B model)\n",
    "# - model.Q5_K_M.gguf  (~1.2GB for 1.7B model)\n",
    "# - README.md (usage instructions)\n",
    "# - tokenizer files\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Using Your GGUF Model\n",
    "\n",
    "**With Ollama:**\n",
    "```bash\n",
    "cd outputs/{model_dir}/gguf\n",
    "\n",
    "# Create Ollama model\n",
    "cat > Modelfile <<EOF\n",
    "FROM ./model.Q4_K_M.gguf\n",
    "PARAMETER temperature 0.7\n",
    "PARAMETER top_p 0.9\n",
    "EOF\n",
    "\n",
    "ollama create {model_dir.lower()} -f Modelfile\n",
    "ollama run {model_dir.lower()} \"Hello! How can you help me?\"\n",
    "```\n",
    "\n",
    "**With llama.cpp:**\n",
    "```bash\n",
    "# Interactive mode\n",
    "./llama.cpp/llama-cli -m outputs/{model_dir}/gguf/model.Q4_K_M.gguf \\\n",
    "  -p \"Hello! How can you help me?\" \\\n",
    "  --temp 0.7\n",
    "\n",
    "# Server mode\n",
    "./llama.cpp/llama-server -m outputs/{model_dir}/gguf/model.Q4_K_M.gguf \\\n",
    "  --host 0.0.0.0 --port 8080\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Optional: Upload GGUF to HuggingFace\n",
    "\n",
    "After creating GGUF files, you can upload them to a separate repository:\n",
    "\n",
    "```bash\n",
    "# Login to HuggingFace (one-time)\n",
    "hf auth login\n",
    "\n",
    "# Create GGUF repository\n",
    "hf repo create {model_dir}-gguf --type model\n",
    "\n",
    "# Upload GGUF files\n",
    "hf upload {HF_USERNAME if HF_USERNAME else \"your-username\"}/{model_dir}-gguf \\\n",
    "  outputs/{model_dir}/gguf \\\n",
    "  --repo-type model\n",
    "```\n",
    "\n",
    "Your GGUF models will be at: `https://huggingface.co/{HF_USERNAME if HF_USERNAME else \"your-username\"}/{model_dir}-gguf`\n",
    "\n",
    "---\n",
    "\n",
    "### Available GGUF Quantizations\n",
    "\n",
    "Edit `OUTPUT_FORMATS` in the `.env` file above to choose quantizations:\n",
    "\n",
    "| Format | Size (1.7B) | Quality | Use Case |\n",
    "|--------|-------------|---------|----------|\n",
    "| `gguf_q4_k_m` | ~1.0GB | Good | **Recommended** - best balance |\n",
    "| `gguf_q5_k_m` | ~1.2GB | Better | Higher quality, larger size |\n",
    "| `gguf_q8_0` | ~1.8GB | Excellent | Near original quality |\n",
    "| `gguf_f16` | ~3.4GB | Best | Full precision (largest) |\n",
    "\n",
    "Example for multiple formats:\n",
    "```bash\n",
    "OUTPUT_FORMATS=gguf_q4_k_m,gguf_q5_k_m,gguf_q8_0\n",
    "```"
   ],
   "metadata": {},
   "id": "2868669a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Test Your Model (Optional)\n",
    "\n",
    "Quick test of your fine-tuned model:"
   ],
   "id": "6537b358"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model_path = f\"outputs/{model_dir}/lora\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_path,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test prompt\n",
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "What is machine learning?\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, temperature=0.7)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL RESPONSE:\")\n",
    "print(\"=\"*50)\n",
    "print(response)\n",
    "print(\"=\"*50)"
   ],
   "id": "2512aae5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Done!\n",
    "\n",
    "Your model has been trained and is ready to use!\n",
    "\n",
    "**Next steps:**\n",
    "1. Download the model from Google Drive or HuggingFace\n",
    "2. Use it locally with Ollama or transformers\n",
    "3. Share it on HuggingFace Hub\n",
    "\n",
    "**Resources:**\n",
    "- [Documentation](https://github.com/farhan-syah/unsloth-finetuning/tree/main/docs)\n",
    "- [Training Guide](https://github.com/farhan-syah/unsloth-finetuning/blob/main/docs/TRAINING.md)\n",
    "- [FAQ](https://github.com/farhan-syah/unsloth-finetuning/blob/main/docs/FAQ.md)"
   ],
   "id": "fb3ac790"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}