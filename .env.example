# Unsloth Fine-tuning Configuration
# Copy this to .env: cp .env.example .env
# This is pre-configured for quick testing - adjust for production use

# ==============================================================================
# MODEL SELECTION
# ==============================================================================

# LORA_BASE_MODEL: Model used for training (REQUIRED)
# Use 4-bit quantized models for memory efficiency
# Choose based on your available VRAM
LORA_BASE_MODEL=unsloth/Qwen3-1.7B-unsloth-bnb-4bit

# Available models:
#   unsloth/Qwen3-1.7B-unsloth-bnb-4bit            (4-6GB VRAM)  ‚Üê Recommended for testing
#   unsloth/Qwen3-VL-2B-Instruct-unsloth-bnb-4bit  (6-8GB VRAM)
#   unsloth/Qwen3-4B-unsloth-bnb-4bit              (12GB VRAM)
#   unsloth/Qwen3-8B-unsloth-bnb-4bit              (24GB VRAM)
#   unsloth/Gemma-3n-E2B-unsloth-bnb-4bit          (8GB VRAM)
# Browse more: https://huggingface.co/unsloth

# INFERENCE_BASE_MODEL: Model used for merging (OPTIONAL)
# Leave empty to use LORA_BASE_MODEL (recommended for most cases)
# Set to original unquantized model for best quality (requires 15-30GB VRAM during build)
INFERENCE_BASE_MODEL=

# Example for true 16-bit quality merging:
#   INFERENCE_BASE_MODEL=unsloth/Qwen3-1.7B
# Note: Must be architecturally compatible with LORA_BASE_MODEL

# OUTPUT_MODEL_NAME: Name for output directories (OPTIONAL)
# "auto" generates: {model_name}-{dataset_name}
# Or set custom name: "my-chatbot-v1"
OUTPUT_MODEL_NAME=auto

# ==============================================================================
# DATASET CONFIGURATION
# ==============================================================================

# DATASET_NAME: HuggingFace dataset to use for training
DATASET_NAME=yahma/alpaca-cleaned

# Popular alternatives:
#   yahma/alpaca-cleaned                        - Clean instruction-following (52K)
#   OpenAssistant/oasst1                        - Multilingual conversations (88K)
#   tatsu-lab/alpaca                            - Original Alpaca dataset
#   timdettmers/openassistant-guanaco           - High-quality instructions

# DATASET_MAX_SAMPLES: Limit dataset size for testing
# 0 = use all samples, >0 = use only first N samples
DATASET_MAX_SAMPLES=100  # Set to 0 for full training

# MAX_STEPS: Override training steps
# 0 = train for NUM_TRAIN_EPOCHS, >0 = stop after N steps
MAX_STEPS=50  # Set to 0 for full training

# ==============================================================================
# TRAINING HYPERPARAMETERS
# ==============================================================================

# MAX_SEQ_LENGTH: Maximum sequence length in tokens
# Controls max tokens per training sample (not model's max context!)
# Modern models support 8k-32k+ context, but longer = quadratic VRAM usage
# Choose based on your dataset and VRAM:
#   512-1024: Short instructions, low VRAM
#   2048-4096: Standard conversations, balanced (recommended)
#   8192+: Long context, requires significant VRAM
# Reduce if OOM errors occur
MAX_SEQ_LENGTH=4096

# LORA_RANK: Rank of LoRA adaptation matrices (r)
# Controls trainable parameters in LoRA adapter matrices.
# Higher rank = more capacity but slower and more memory usage
# Too large can cause overfitting and harm quality
# Common values: 8, 16, 32, 64, 128
# Recommended: 8 or 16 (fast fine-tunes), up to 64 or 128 (complex tasks)
LORA_RANK=16

# LORA_ALPHA: LoRA scaling factor
# Scales the strength of fine-tuned adjustments relative to rank (r)
# Standard: r (same as rank) for baseline
# Aggressive: r * 2 (popular heuristic, makes model learn more aggressively)
# Recommended: r (16) or r * 2 (32) for LORA_RANK=16
LORA_ALPHA=32

# LORA_DROPOUT: Regularization via random activation zeroing
# Sets fraction of LoRA activations to zero during training
# Recent research: unreliable regularizer for short training runs
# ü¶• Unsloth optimizes training when lora_dropout=0 (faster)
# Recommended: 0 (optimized) or 0.05-0.1 if overfitting suspected
# (Note: Configured in train.py, default is 0)

# BIAS: Train bias terms in linear layers
# "none" = don't train bias (faster, less memory, recommended)
# Options: "none", "all", "lora_only"
# ü¶• Unsloth optimizes when bias="none"
# (Note: Configured in train.py, default is "none")

# TARGET_MODULES: Which model layers to apply LoRA to
# Attention layers: q_proj, k_proj, v_proj, o_proj
# MLP layers: gate_proj, up_proj, down_proj
# ü¶• Research shows targeting ALL major layers is crucial for quality
# Removing modules saves minimal memory but harms performance
# Recommended: Target all major linear layers
# (Note: Auto-configured in train.py to target all layers)

# USE_RSLORA: Rank-Stabilized LoRA
# False = standard scaling (lora_alpha / r)
# True = rank-stabilized scaling (lora_alpha / sqrt(r))
# Can improve stability for higher ranks
# (Note: Configured in train.py, default is False)

# BATCH_SIZE: Samples per forward/backward pass on GPU
# Primary Driver of VRAM Usage
# Higher = better hardware utilization and speed, but needs more VRAM
# If OOM errors occur, reduce to 1
# Recommended: 2
BATCH_SIZE=2

# GRADIENT_ACCUMULATION_STEPS: Micro-batches before weight update
# Primary Driver of Training Time
# Simulates larger batch_size without using more VRAM
# Higher = more stable training but slower per epoch
# Recommended: 4-8 (for Effective Batch Size of 8-16)
GRADIENT_ACCUMULATION_STEPS=4

# EFFECTIVE BATCH SIZE = BATCH_SIZE √ó GRADIENT_ACCUMULATION_STEPS
# For this config: 2 √ó 4 = 8 (stable for most tasks)
# Target: 4-16 (Recommended: 8-16)
#
# VRAM vs Performance Trade-off Examples (for Effective Batch Size = 16):
#   batch_size=16, gradient_accumulation=1   ‚Üí Most VRAM, fastest
#   batch_size=8,  gradient_accumulation=2   ‚Üí High VRAM, fast
#   batch_size=4,  gradient_accumulation=4   ‚Üí Medium VRAM, medium speed
#   batch_size=2,  gradient_accumulation=8   ‚Üí Low VRAM, slower
#   batch_size=1,  gradient_accumulation=16  ‚Üí Least VRAM, slowest
#
# To avoid OOM: Use smaller batch_size and increase gradient_accumulation  

# LEARNING_RATE: How fast the model learns
# Too high = unstable training, too low = slow convergence
# Recommended: 2e-4 (good default for LoRA fine-tuning)
LEARNING_RATE=2e-4

# NUM_TRAIN_EPOCHS: Number of passes through the dataset
# Only used if MAX_STEPS=0
# Recommended: 1-3
NUM_TRAIN_EPOCHS=1

# WARMUP_STEPS: Gradually increase learning rate at start
# Stabilizes training by easing into the learning rate
# Recommended: 5-10% of total steps
WARMUP_STEPS=5

# PACKING: Pack multiple short sequences into one sequence
# true = faster training for short texts, false = simpler
# Recommended: false for simplicity, true for efficiency with short texts
PACKING=false

# WEIGHT_DECAY: Regularization to prevent overfitting (0.01 recommended)
# Penalizes large weights to improve generalization. Don't use too large!
# Configured via training arguments (default: 0.01)

# SCHEDULER_TYPE: Learning rate adjustment strategy (linear recommended)
# Options: linear, cosine, constant
# Configured in train.py (default: linear)

# ==============================================================================
# OPTIMIZATION SETTINGS
# ==============================================================================

# USE_GRADIENT_CHECKPOINTING: Trade compute for memory
# Options: true, false, "unsloth"
# true = standard gradient checkpointing (lower VRAM, slower)
# false = no checkpointing (faster, more VRAM)
# "unsloth" = ü¶• 30% extra memory savings, supports very long context
# Recommended: "unsloth" for long context, false for speed
USE_GRADIENT_CHECKPOINTING=true

# MAX_GRAD_NORM: Gradient clipping threshold
# Prevents exploding gradients. 1.0 is standard.
MAX_GRAD_NORM=1.0

# OPTIM: Optimizer to use
# adamw_8bit = memory-efficient (recommended)
# adamw_torch = standard PyTorch optimizer (more VRAM)
OPTIM=adamw_8bit

# ==============================================================================
# LOGGING & CHECKPOINTS
# ==============================================================================

# LOGGING_STEPS: Log training metrics every N steps
# Lower = more frequent logging (useful for monitoring)
LOGGING_STEPS=5

# SAVE_STEPS: Save checkpoint every N steps
# Lower = more frequent saves (safer but slower)
SAVE_STEPS=25

# SAVE_TOTAL_LIMIT: Keep only N most recent checkpoints
# Older checkpoints are automatically deleted
SAVE_TOTAL_LIMIT=2

# SAVE_ONLY_FINAL: Skip intermediate checkpoints
# true = faster training, saves disk space (recommended for testing)
# false = save checkpoints during training (safer for long runs)
SAVE_ONLY_FINAL=true

# ==============================================================================
# MONITORING (Optional)
# ==============================================================================

# WANDB_ENABLED: Enable Weights & Biases tracking
# Requires wandb account: https://wandb.ai
WANDB_ENABLED=false
WANDB_PROJECT=unsloth-finetuning
WANDB_RUN_NAME=auto  # Auto-generates from model name

# ==============================================================================
# OUTPUT FORMATS
# ==============================================================================

# OUTPUT_FORMATS: Additional formats to create (comma-separated)
# train.py creates: lora/
# build.py creates: merged_16bit/ + formats listed here
#
# Available formats:
#   merged_4bit    - 4-bit safetensors (smaller, for HuggingFace)
#   gguf_f16       - 16-bit GGUF (largest, best quality)
#   gguf_q8_0      - 8-bit GGUF (high quality)
#   gguf_q5_k_m    - 5-bit GGUF (good quality)
#   gguf_q4_k_m    - 4-bit GGUF (best balance) ‚Üê Recommended
#   gguf_q2_k      - 2-bit GGUF (smallest, lower quality)
#
# All GGUF quantizations go into a single gguf/ folder
OUTPUT_FORMATS=gguf_q4_k_m  # Set to empty for none

# ==============================================================================
# HUGGINGFACE HUB (Optional)
# ==============================================================================

# PUSH_TO_HUB: Automatically upload model to HuggingFace
PUSH_TO_HUB=false
HF_USERNAME=your_username
HF_MODEL_NAME=auto  # Auto-generates from model name
HF_TOKEN=  # Get from: https://huggingface.co/settings/tokens

# ==============================================================================
# AUTHOR ATTRIBUTION
# ==============================================================================

# AUTHOR_NAME: Your name for model card and citations
# Used in generated README files and BibTeX citations
# This identifies YOU as the person who trained/fine-tuned the model
# The README will also credit the training pipeline and Unsloth
AUTHOR_NAME=Your Name

# ==============================================================================
# ADVANCED SETTINGS
# ==============================================================================

# OUTPUT_DIR_BASE: Base directory for all outputs
OUTPUT_DIR_BASE=./outputs

# PREPROCESSED_DATA_DIR: Cache directory for preprocessed datasets
PREPROCESSED_DATA_DIR=./data/preprocessed

# CACHE_DIR: HuggingFace cache directory
CACHE_DIR=./cache

# SEED: Random seed for reproducibility
# Fixed number ensures identical results across runs with same data/config
# Useful for debugging and comparing experiments
# Recommended: Any integer (e.g., 42, 3407)
SEED=3407

# FORCE_PREPROCESS: Reprocess dataset even if cached
# Set true after changing MAX_SEQ_LENGTH or dataset processing
FORCE_PREPROCESS=false

# FORCE_RETRAIN: Retrain even if LoRA adapters exist
FORCE_RETRAIN=true  # Set to false for production

# FORCE_REBUILD: Rebuild formats even if they exist
FORCE_REBUILD=true  # Set to false for production

# CHECK_SEQ_LENGTH: Check and filter samples exceeding MAX_SEQ_LENGTH
# true = safer but slower preprocessing (shows statistics)
# false = faster, trainer handles truncation automatically (recommended)
CHECK_SEQ_LENGTH=false

# ==============================================================================
# QUICK START GUIDE
# ==============================================================================
#
# 1. Copy this file:
#    cp .env.example .env
#
# 2. Quick test (~7 minutes total):
#    python train.py   # ~2 min: Creates lora/
#    python build.py   # ~5 min: Creates merged_16bit/ and gguf/
#
# 3. For production training:
#    - Set MAX_STEPS=0
#    - Set DATASET_MAX_SAMPLES=0
#    - Set LORA_RANK=64
#    - Set LORA_ALPHA=128
#    - Set GRADIENT_ACCUMULATION_STEPS=4
#    - Set SAVE_ONLY_FINAL=false
#    - Set FORCE_RETRAIN=false
#    - Set FORCE_REBUILD=false
#
# 4. Outputs will be in:
#    outputs/Qwen3-VL-2B-Instruct-alpaca-cleaned/
#    ‚îú‚îÄ‚îÄ lora/           (LoRA adapters)
#    ‚îú‚îÄ‚îÄ merged_16bit/   (Merged model + Modelfile)
#    ‚îî‚îÄ‚îÄ gguf/           (GGUF quantizations)
#
# 5. Use with Ollama:
#    cd outputs/Qwen3-VL-2B-Instruct-alpaca-cleaned/merged_16bit
#    ollama create my-model -f Modelfile
#    ollama run my-model "Hello!"
#
# ==============================================================================
