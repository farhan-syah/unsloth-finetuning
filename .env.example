# Unsloth Fine-tuning Configuration
# Copy this to .env: cp .env.example .env
# This is pre-configured for quick testing - adjust for production use
#
# ü§ñ AUTO-DETECTION FEATURES:
# - Chat templates auto-detected from model name (llama-3.1, qwen2.5, phi-3, etc.)
# - Dataset formats auto-detected (conversations, ShareGPT, instruction-output)
# - Model capabilities auto-detected (reasoning models, instruct vs base)
# - Smart recommendations from preprocess.py (batch size, max steps)

# ==============================================================================
# MODEL SELECTION
# ==============================================================================

# LORA_BASE_MODEL: Model used for training (REQUIRED)
# For small datasets (1K-5K samples), use Instruct models to prevent overfitting
# For large datasets (10K+ samples), base models work well
# Chat template is auto-detected: Llama uses llama-3.1, Qwen uses qwen2.5, etc.
LORA_BASE_MODEL=unsloth/Llama-3.2-1B-Instruct-bnb-4bit

# Available models:
#   unsloth/Llama-3.2-1B-Instruct-bnb-4bit         (4-6GB VRAM)  ‚Üê Recommended for small datasets
#   unsloth/Llama-3.2-3B-Instruct-bnb-4bit         (6-8GB VRAM)
#   unsloth/Phi-3.5-mini-instruct-bnb-4bit         (6-8GB VRAM)
#   unsloth/gemma-2-2b-it-bnb-4bit                 (6-8GB VRAM)
#   unsloth/Qwen2.5-1.5B-Instruct-bnb-4bit         (4-6GB VRAM)
#
# Base models (use only with large datasets >10K samples):
#   unsloth/Llama-3.2-1B-bnb-4bit                  (4-6GB VRAM)
#   unsloth/Llama-3.2-3B-bnb-4bit                  (6-8GB VRAM)
#
# Browse more: https://huggingface.co/unsloth

# INFERENCE_BASE_MODEL: Model used for merging (OPTIONAL)
# REQUIRED for GGUF conversion - must be unquantized (not bnb-4bit)
# Leave empty to skip GGUF conversion (only creates merged safetensors)
INFERENCE_BASE_MODEL=unsloth/Llama-3.2-1B-Instruct

# Note: Must be unquantized and architecturally compatible with LORA_BASE_MODEL
# Examples:
#   Training on: unsloth/Llama-3.2-1B-Instruct-bnb-4bit
#   Merge into:  unsloth/Llama-3.2-1B-Instruct (unquantized)

# OUTPUT_MODEL_NAME: Name for output directories (OPTIONAL)
# "auto" generates: {model_name}-{dataset_name}
# Or set custom name: "my-chatbot-v1"
OUTPUT_MODEL_NAME=auto

# ==============================================================================
# DATASET CONFIGURATION
# ==============================================================================

# DATASET_NAME: HuggingFace dataset to use for training
DATASET_NAME=your-dataset/name

# Dataset examples by type:
#   High-quality curated (1K-5K):
#     GAIR/lima - 1K samples (gated, requires HF login)
#   Large instruction (10K-100K):
#     yahma/alpaca-cleaned - 52K samples (ungated)
#     databricks/databricks-dolly-15k - 15K samples (ungated)
#   Domain-specific: Medical, legal, code datasets

# DATASET_MAX_SAMPLES: Limit dataset size for testing
# 0 = use all samples, >0 = use only first N samples
# Useful for quick testing with large datasets
DATASET_MAX_SAMPLES=0  # Use all samples (recommended for production)

# MAX_STEPS: Override training steps
# 0 = train for NUM_TRAIN_EPOCHS, >0 = stop after N steps
# Use MAX_STEPS > 0 for quick testing only
MAX_STEPS=0  # Use full epochs (recommended for production)

# ==============================================================================
# TRAINING HYPERPARAMETERS
# ==============================================================================

# MAX_SEQ_LENGTH: Maximum sequence length in tokens
# Longer = more context but more VRAM. Must match model's max length.
# Reduce if OOM errors occur.
MAX_SEQ_LENGTH=4096

# LORA_RANK: Rank of LoRA adaptation matrices (r)
# Controls trainable parameters in LoRA adapter matrices.
# Higher rank = more capacity but slower and more memory usage
# Too large can cause overfitting and harm quality
# Common values: 8, 16, 32, 64, 128
# Recommended: 8 or 16 (fast fine-tunes), up to 64 or 128 (complex tasks)
LORA_RANK=16

# LORA_ALPHA: LoRA scaling factor
# Scales the strength of fine-tuned adjustments relative to rank (r)
# Standard: r (same as rank) for baseline
# Aggressive: r * 2 (popular heuristic, makes model learn more aggressively)
# Recommended: r (16) or r * 2 (32) for LORA_RANK=16
LORA_ALPHA=32

# LORA_DROPOUT: Regularization via random activation zeroing
# Sets fraction of LoRA activations to zero during training
# Recent research: unreliable regularizer for short training runs
# ü¶• Unsloth optimizes training when lora_dropout=0 (faster)
# Recommended: 0 (optimized) or 0.05-0.1 if overfitting suspected
# (Note: Configured in train.py, default is 0)

# BIAS: Train bias terms in linear layers
# "none" = don't train bias (faster, less memory, recommended)
# Options: "none", "all", "lora_only"
# ü¶• Unsloth optimizes when bias="none"
# (Note: Configured in train.py, default is "none")

# TARGET_MODULES: Which model layers to apply LoRA to
# Attention layers: q_proj, k_proj, v_proj, o_proj
# MLP layers: gate_proj, up_proj, down_proj
# ü¶• Research shows targeting ALL major layers is crucial for quality
# Removing modules saves minimal memory but harms performance
# Recommended: Target all major linear layers
# (Note: Auto-configured in train.py to target all layers)

# USE_RSLORA: Rank-Stabilized LoRA
# False = standard scaling (lora_alpha / r)
# True = rank-stabilized scaling (lora_alpha / sqrt(r))
# Can improve stability for higher ranks
# (Note: Configured in train.py, default is False)

# BATCH_SIZE: Samples per forward/backward pass on GPU
# Primary Driver of VRAM Usage
# Higher = better hardware utilization and speed, but needs more VRAM
# If OOM errors occur, reduce to 1
# Recommended: 2
BATCH_SIZE=2

# GRADIENT_ACCUMULATION_STEPS: Micro-batches before weight update
# Primary Driver of Training Time
# Simulates larger batch_size without using more VRAM
# Higher = more stable training but slower per epoch
# Recommended: 4-8 (for Effective Batch Size of 8-16)
GRADIENT_ACCUMULATION_STEPS=4

# EFFECTIVE BATCH SIZE = BATCH_SIZE √ó GRADIENT_ACCUMULATION_STEPS
# For this config: 2 √ó 4 = 8 (stable for most tasks)
# Target: 4-16 (Recommended: 8-16)
#
# VRAM vs Performance Trade-off Examples (for Effective Batch Size = 16):
#   batch_size=16, gradient_accumulation=1   ‚Üí Most VRAM, fastest
#   batch_size=8,  gradient_accumulation=2   ‚Üí High VRAM, fast
#   batch_size=4,  gradient_accumulation=4   ‚Üí Medium VRAM, medium speed
#   batch_size=2,  gradient_accumulation=8   ‚Üí Low VRAM, slower
#   batch_size=1,  gradient_accumulation=16  ‚Üí Least VRAM, slowest
#
# To avoid OOM: Use smaller batch_size and increase gradient_accumulation  

# LEARNING_RATE: How fast the model learns
# Too high = unstable training, too low = slow convergence
# Recommended: 2e-4 (good default for LoRA fine-tuning)
LEARNING_RATE=2e-4

# NUM_TRAIN_EPOCHS: Number of passes through the dataset
# Only used if MAX_STEPS=0
# Recommended: 1-3
NUM_TRAIN_EPOCHS=1

# WARMUP_STEPS: Gradually increase learning rate at start
# Stabilizes training by easing into the learning rate
# Recommended: 5-10% of total steps
WARMUP_STEPS=5

# PACKING: Pack multiple short sequences into one sequence
# true = faster training for short texts, false = simpler
# Recommended: false for simplicity, true for efficiency with short texts
PACKING=false

# WEIGHT_DECAY: Regularization to prevent overfitting (0.01 recommended)
# Penalizes large weights to improve generalization. Don't use too large!
# Configured via training arguments (default: 0.01)

# SCHEDULER_TYPE: Learning rate adjustment strategy (linear recommended)
# Options: linear, cosine, constant
# Configured in train.py (default: linear)

# ==============================================================================
# OPTIMIZATION SETTINGS
# ==============================================================================

# USE_GRADIENT_CHECKPOINTING: Trade compute for memory
# Options: true, false, "unsloth"
# true = standard gradient checkpointing (lower VRAM, slower)
# false = no checkpointing (faster, more VRAM)
# "unsloth" = ü¶• 30% extra memory savings, supports very long context
# Recommended: "unsloth" for long context, false for speed
USE_GRADIENT_CHECKPOINTING=true

# MAX_GRAD_NORM: Gradient clipping threshold
# Prevents exploding gradients. 1.0 is standard.
MAX_GRAD_NORM=1.0

# OPTIM: Optimizer to use
# adamw_8bit = memory-efficient (recommended)
# adamw_torch = standard PyTorch optimizer (more VRAM)
OPTIM=adamw_8bit

# ==============================================================================
# LOGGING & CHECKPOINTS
# ==============================================================================

# LOGGING_STEPS: Log training metrics every N steps
# Lower = more frequent logging (useful for monitoring)
LOGGING_STEPS=5

# SAVE_STEPS: Save checkpoint every N steps
# Lower = more frequent saves (safer but slower)
SAVE_STEPS=25

# SAVE_TOTAL_LIMIT: Keep only N most recent checkpoints
# Older checkpoints are automatically deleted
SAVE_TOTAL_LIMIT=2

# SAVE_ONLY_FINAL: Skip intermediate checkpoints
# true = faster training, saves disk space (recommended for testing)
# false = save checkpoints during training (safer for long runs)
SAVE_ONLY_FINAL=true

# ==============================================================================
# MONITORING (Optional)
# ==============================================================================

# WANDB_ENABLED: Enable Weights & Biases tracking
# Requires wandb account: https://wandb.ai
WANDB_ENABLED=false
WANDB_PROJECT=unsloth-finetuning
WANDB_RUN_NAME=auto  # Auto-generates from model name

# ==============================================================================
# OUTPUT FORMATS
# ==============================================================================

# OUTPUT_FORMATS: Additional formats to create (comma-separated)
# train.py creates: lora/
# build.py creates: merged_16bit/ + formats listed here
#
# Available formats:
#   merged_4bit    - 4-bit safetensors (smaller, for HuggingFace)
#   gguf_f16       - 16-bit GGUF (largest, best quality)
#   gguf_q8_0      - 8-bit GGUF (high quality)
#   gguf_q5_k_m    - 5-bit GGUF (good quality)
#   gguf_q4_k_m    - 4-bit GGUF (best balance) ‚Üê Recommended
#   gguf_q2_k      - 2-bit GGUF (smallest, lower quality)
#
# All GGUF quantizations go into a single gguf/ folder
OUTPUT_FORMATS=gguf_q4_k_m  # Set to empty for none

# ==============================================================================
# HUGGINGFACE HUB (Optional)
# ==============================================================================

# PUSH_TO_HUB: Automatically upload model to HuggingFace
PUSH_TO_HUB=false
HF_USERNAME=  # Your HuggingFace username
HF_MODEL_NAME=auto  # Auto-generates from model name
HF_TOKEN=  # Get from: https://huggingface.co/settings/tokens

# ==============================================================================
# AUTHOR ATTRIBUTION
# ==============================================================================

# AUTHOR_NAME: Your name for model card and citations
# Used in generated README files and BibTeX citations
# This identifies YOU as the person who trained/fine-tuned the model
# The README will also credit the training pipeline and Unsloth
AUTHOR_NAME=  # Your name or username

# ==============================================================================
# ADVANCED SETTINGS
# ==============================================================================

# OUTPUT_DIR_BASE: Base directory for all outputs
OUTPUT_DIR_BASE=./outputs

# PREPROCESSED_DATA_DIR: Cache directory for preprocessed datasets
PREPROCESSED_DATA_DIR=./data/preprocessed

# CACHE_DIR: HuggingFace cache directory
CACHE_DIR=./cache

# SEED: Random seed for reproducibility
# Fixed number ensures identical results across runs with same data/config
# Useful for debugging and comparing experiments
# Recommended: Any integer (e.g., 42, 3407)
SEED=3407

# CHECK_SEQ_LENGTH: Check and filter samples exceeding MAX_SEQ_LENGTH during preprocessing
# Used by preprocess.py (hybrid approach: char estimation + selective tokenization)
# true = analyze and filter long samples, provide statistics (recommended)
# false = skip length checking, trainer truncates automatically
CHECK_SEQ_LENGTH=true

# FORCE_PREPROCESS: Reprocess dataset even if already preprocessed
# Set true after changing MAX_SEQ_LENGTH or dataset
FORCE_PREPROCESS=false

# FORCE_RETRAIN: Retrain even if LoRA adapters exist
FORCE_RETRAIN=true  # Set to false for production

# FORCE_REBUILD: Rebuild formats even if they exist
FORCE_REBUILD=true  # Set to false for production

# ==============================================================================
# QUICK START GUIDE
# ==============================================================================
#
# 1. Copy this file:
#    cp .env.example .env
#
# 2. Quick test (~10 minutes total):
#    python scripts/preprocess.py  # ~1 min: Analyze dataset, get config recommendations
#    python scripts/train.py       # ~2 min: Creates lora/
#    python scripts/build.py       # ~5 min: Creates merged_16bit/ and gguf/
#
# 3. For production training:
#    a) Run scripts/preprocess.py first:
#       - Analyzes your dataset
#       - Recommends optimal BATCH_SIZE, MAX_STEPS for 1-3 epochs
#       - Shows GPU memory and sequence length stats
#
#    b) Update .env with recommended settings, then:
#       - Set MAX_STEPS=0 (or use recommended value from scripts/preprocess.py)
#       - Set DATASET_MAX_SAMPLES=0
#       - Set LORA_RANK=64
#       - Set LORA_ALPHA=128
#       - Set GRADIENT_ACCUMULATION_STEPS=4 (or use recommended)
#       - Set SAVE_ONLY_FINAL=false
#       - Set FORCE_RETRAIN=false
#       - Set FORCE_REBUILD=false
#
# 4. Outputs will be in:
#    outputs/Qwen3-VL-2B-Instruct-alpaca-cleaned/
#    ‚îú‚îÄ‚îÄ lora/           (LoRA adapters)
#    ‚îú‚îÄ‚îÄ merged_16bit/   (Merged model + Modelfile)
#    ‚îî‚îÄ‚îÄ gguf/           (GGUF quantizations)
#
# 5. Use with Ollama:
#    cd outputs/Qwen3-VL-2B-Instruct-alpaca-cleaned/merged_16bit
#    ollama create my-model -f Modelfile
#    ollama run my-model "Hello!"
#
# ==============================================================================
