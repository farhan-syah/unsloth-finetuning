# Unsloth Fine-tuning Configuration
# Copy this to .env: cp .env.example .env
# This is pre-configured for quick testing - adjust for production use

# ==============================================================================
# MODEL SELECTION
# ==============================================================================

# LORA_BASE_MODEL: Model used for training (REQUIRED)
# Use 4-bit quantized models for memory efficiency
# Choose based on your available VRAM
LORA_BASE_MODEL=unsloth/Qwen3-1.7B-unsloth-bnb-4bit

# Available models:
#   unsloth/Qwen3-1.7B-unsloth-bnb-4bit            (4-6GB VRAM)  ← Recommended for testing
#   unsloth/Qwen3-VL-2B-Instruct-unsloth-bnb-4bit  (6-8GB VRAM)
#   unsloth/Qwen3-4B-unsloth-bnb-4bit              (12GB VRAM)
#   unsloth/Qwen3-8B-unsloth-bnb-4bit              (24GB VRAM)
#   unsloth/Gemma-3n-E2B-unsloth-bnb-4bit          (8GB VRAM)
# Browse more: https://huggingface.co/unsloth

# INFERENCE_BASE_MODEL: Model used for merging (OPTIONAL)
# Leave empty to use LORA_BASE_MODEL (recommended for most cases)
# Set to original unquantized model for best quality (requires 15-30GB VRAM during build)
INFERENCE_BASE_MODEL=

# Example for true 16-bit quality merging:
#   INFERENCE_BASE_MODEL=Qwen/Qwen2.5-1.7B
# Note: Must be architecturally compatible with LORA_BASE_MODEL

# OUTPUT_MODEL_NAME: Name for output directories (OPTIONAL)
# "auto" generates: {model_name}-{dataset_name}
# Or set custom name: "my-chatbot-v1"
OUTPUT_MODEL_NAME=auto

# ==============================================================================
# DATASET CONFIGURATION
# ==============================================================================

# DATASET_NAME: HuggingFace dataset to use for training
DATASET_NAME=yahma/alpaca-cleaned

# Popular alternatives:
#   yahma/alpaca-cleaned                        - Clean instruction-following (52K)
#   OpenAssistant/oasst1                        - Multilingual conversations (88K)
#   tatsu-lab/alpaca                            - Original Alpaca dataset
#   timdettmers/openassistant-guanaco           - High-quality instructions

# DATASET_MAX_SAMPLES: Limit dataset size for testing
# 0 = use all samples, >0 = use only first N samples
DATASET_MAX_SAMPLES=100  # Set to 0 for full training

# MAX_STEPS: Override training steps
# 0 = train for NUM_TRAIN_EPOCHS, >0 = stop after N steps
MAX_STEPS=50  # Set to 0 for full training

# ==============================================================================
# TRAINING HYPERPARAMETERS
# ==============================================================================

# MAX_SEQ_LENGTH: Maximum sequence length in tokens
# Longer = more context but more VRAM. Must match model's max length.
# Reduce if OOM errors occur.
MAX_SEQ_LENGTH=2048

# LORA_RANK: Rank of LoRA adaptation matrices
# Higher = better quality but larger adapters and more VRAM
# Common values: 16 (testing), 32 (low), 64 (standard), 128 (high quality)
LORA_RANK=16  # Use 64 for production

# LORA_ALPHA: LoRA scaling factor
# Typically 2x LORA_RANK. Controls adaptation strength.
LORA_ALPHA=32  # Use 128 for production

# BATCH_SIZE: Number of samples per training step
# Higher = faster training but more VRAM
# Reduce to 1 if OOM errors occur
BATCH_SIZE=2

# GRADIENT_ACCUMULATION_STEPS: Accumulate gradients over N steps
# Effective batch size = BATCH_SIZE × GRADIENT_ACCUMULATION_STEPS
# Increase this instead of BATCH_SIZE to save VRAM
GRADIENT_ACCUMULATION_STEPS=2  # Use 4 for production

# LEARNING_RATE: How fast the model learns
# Too high = unstable, too low = slow convergence
# 2e-4 is a good default for LoRA fine-tuning
LEARNING_RATE=2e-4

# NUM_TRAIN_EPOCHS: Number of passes through the dataset
# Only used if MAX_STEPS=0
NUM_TRAIN_EPOCHS=1

# WARMUP_STEPS: Gradually increase learning rate over N steps
# Helps training stability. Typically 5-10% of total steps.
WARMUP_STEPS=2  # Use 5 for production

# PACKING: Pack multiple short sequences into one sequence
# true = faster training for short texts, false = simpler (recommended)
PACKING=false

# ==============================================================================
# OPTIMIZATION SETTINGS
# ==============================================================================

# USE_GRADIENT_CHECKPOINTING: Trade compute for memory
# true = lower VRAM usage but slower training
# false = faster but needs more VRAM
USE_GRADIENT_CHECKPOINTING=true

# MAX_GRAD_NORM: Gradient clipping threshold
# Prevents exploding gradients. 1.0 is standard.
MAX_GRAD_NORM=1.0

# OPTIM: Optimizer to use
# adamw_8bit = memory-efficient (recommended)
# adamw_torch = standard PyTorch optimizer (more VRAM)
OPTIM=adamw_8bit

# ==============================================================================
# LOGGING & CHECKPOINTS
# ==============================================================================

# LOGGING_STEPS: Log training metrics every N steps
# Lower = more frequent logging (useful for monitoring)
LOGGING_STEPS=5

# SAVE_STEPS: Save checkpoint every N steps
# Lower = more frequent saves (safer but slower)
SAVE_STEPS=25

# SAVE_TOTAL_LIMIT: Keep only N most recent checkpoints
# Older checkpoints are automatically deleted
SAVE_TOTAL_LIMIT=2

# SAVE_ONLY_FINAL: Skip intermediate checkpoints
# true = faster training, saves disk space (recommended for testing)
# false = save checkpoints during training (safer for long runs)
SAVE_ONLY_FINAL=true

# ==============================================================================
# MONITORING (Optional)
# ==============================================================================

# WANDB_ENABLED: Enable Weights & Biases tracking
# Requires wandb account: https://wandb.ai
WANDB_ENABLED=false
WANDB_PROJECT=unsloth-finetuning
WANDB_RUN_NAME=auto  # Auto-generates from model name

# ==============================================================================
# OUTPUT FORMATS
# ==============================================================================

# OUTPUT_FORMATS: Additional formats to create (comma-separated)
# train.py creates: lora/
# build.py creates: merged_16bit/ + formats listed here
#
# Available formats:
#   merged_4bit    - 4-bit safetensors (smaller, for HuggingFace)
#   gguf_f16       - 16-bit GGUF (largest, best quality)
#   gguf_q8_0      - 8-bit GGUF (high quality)
#   gguf_q5_k_m    - 5-bit GGUF (good quality)
#   gguf_q4_k_m    - 4-bit GGUF (best balance) ← Recommended
#   gguf_q2_k      - 2-bit GGUF (smallest, lower quality)
#
# All GGUF quantizations go into a single gguf/ folder
OUTPUT_FORMATS=gguf_q4_k_m  # Set to empty for none

# ==============================================================================
# HUGGINGFACE HUB (Optional)
# ==============================================================================

# PUSH_TO_HUB: Automatically upload model to HuggingFace
PUSH_TO_HUB=false
HF_USERNAME=your_username
HF_MODEL_NAME=auto  # Auto-generates from model name
HF_TOKEN=  # Get from: https://huggingface.co/settings/tokens

# ==============================================================================
# AUTHOR ATTRIBUTION
# ==============================================================================

# AUTHOR_NAME: Your name for model card and citations
# Used in generated README files and BibTeX citations
# This identifies YOU as the person who trained/fine-tuned the model
# The README will also credit the training pipeline and Unsloth
AUTHOR_NAME=Your Name

# ==============================================================================
# ADVANCED SETTINGS
# ==============================================================================

# OUTPUT_DIR_BASE: Base directory for all outputs
OUTPUT_DIR_BASE=./outputs

# PREPROCESSED_DATA_DIR: Cache directory for preprocessed datasets
PREPROCESSED_DATA_DIR=./data/preprocessed

# CACHE_DIR: HuggingFace cache directory
CACHE_DIR=./cache

# SEED: Random seed for reproducibility
SEED=3407

# FORCE_PREPROCESS: Reprocess dataset even if cached
# Set true after changing MAX_SEQ_LENGTH or dataset processing
FORCE_PREPROCESS=false

# FORCE_RETRAIN: Retrain even if LoRA adapters exist
FORCE_RETRAIN=true  # Set to false for production

# FORCE_REBUILD: Rebuild formats even if they exist
FORCE_REBUILD=true  # Set to false for production

# CHECK_SEQ_LENGTH: Check and filter samples exceeding MAX_SEQ_LENGTH
# true = safer but slower preprocessing (shows statistics)
# false = faster, trainer handles truncation automatically (recommended)
CHECK_SEQ_LENGTH=false

# ==============================================================================
# QUICK START GUIDE
# ==============================================================================
#
# 1. Copy this file:
#    cp .env.example .env
#
# 2. Quick test (~7 minutes total):
#    python train.py   # ~2 min: Creates lora/
#    python build.py   # ~5 min: Creates merged_16bit/ and gguf/
#
# 3. For production training:
#    - Set MAX_STEPS=0
#    - Set DATASET_MAX_SAMPLES=0
#    - Set LORA_RANK=64
#    - Set LORA_ALPHA=128
#    - Set GRADIENT_ACCUMULATION_STEPS=4
#    - Set SAVE_ONLY_FINAL=false
#    - Set FORCE_RETRAIN=false
#    - Set FORCE_REBUILD=false
#
# 4. Outputs will be in:
#    outputs/Qwen3-VL-2B-Instruct-alpaca-cleaned/
#    ├── lora/           (LoRA adapters)
#    ├── merged_16bit/   (Merged model + Modelfile)
#    └── gguf/           (GGUF quantizations)
#
# 5. Use with Ollama:
#    cd outputs/Qwen3-VL-2B-Instruct-alpaca-cleaned/merged_16bit
#    ollama create my-model -f Modelfile
#    ollama run my-model "Hello!"
#
# ==============================================================================
