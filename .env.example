# =============================================================================
# Environment Configuration
# =============================================================================
# This file contains credentials, paths, and operational settings.
# Model selection, dataset, and training hyperparameters are in training_params.yaml
#
# Setup:
#   1. Copy this file to .env: cp .env.example .env
#   2. Fill in your credentials (HF_TOKEN, HF_USERNAME, AUTHOR_NAME)
#   3. Configure model/dataset/training in training_params.yaml
#
# What goes where:
#   - training_params.yaml: Model, dataset, training hyperparameters (shareable)
#   - .env: Credentials, tokens, paths, personal settings (private, don't commit)
# =============================================================================

# -----------------------------------------------------------------------------
# Directory Paths
# -----------------------------------------------------------------------------
# Where to store outputs, preprocessed data, and caches
OUTPUT_DIR_BASE=./outputs
PREPROCESSED_DATA_DIR=./data/preprocessed
CACHE_DIR=./cache

# -----------------------------------------------------------------------------
# HuggingFace Hub Integration
# -----------------------------------------------------------------------------
# Push models to HuggingFace Hub after training/building

# Enable/disable pushing to hub
PUSH_TO_HUB=false

# Your HuggingFace username
HF_USERNAME=

# Model name on HuggingFace (auto = use OUTPUT_MODEL_NAME)
HF_MODEL_NAME=auto

# HuggingFace access token (get from: https://huggingface.co/settings/tokens)
# IMPORTANT: Keep this secret! Never commit .env to git
HF_TOKEN=

# -----------------------------------------------------------------------------
# Author Attribution
# -----------------------------------------------------------------------------
# Your name for model card and documentation
AUTHOR_NAME=Your Name

# -----------------------------------------------------------------------------
# Weights & Biases Integration
# -----------------------------------------------------------------------------
# Track training runs with W&B (optional)

# Enable/disable W&B logging
WANDB_ENABLED=false

# W&B project name
WANDB_PROJECT=unsloth-finetuning

# W&B run name (auto = generate from model name + lora rank)
WANDB_RUN_NAME=auto

# -----------------------------------------------------------------------------
# Operational Flags
# -----------------------------------------------------------------------------
# Control script behavior (force reprocessing, rebuilding, etc.)

# Check sequence lengths during preprocessing
CHECK_SEQ_LENGTH=true

# Force preprocessing even if cached data exists
FORCE_PREPROCESS=false

# Force rebuilding merged/GGUF models even if they exist
# Note: Training now uses automatic backup system - no force flag needed
FORCE_REBUILD=false

# -----------------------------------------------------------------------------
# Ollama Configuration
# -----------------------------------------------------------------------------
# Ollama server URL for benchmarking with GGUF models
OLLAMA_BASE_URL=http://localhost:11434

# =============================================================================
# TRAINING CONFIGURATION HAS MOVED!
# =============================================================================
# All training hyperparameters are now in training_params.yaml
#
# This includes:
#   - LoRA settings (rank, alpha, dropout, use_rslora)
#   - Batch size and gradient accumulation
#   - Learning rate and optimizer
#   - Number of epochs and max steps
#   - Sequence length and packing
#   - Logging and checkpoint settings
#   - Output formats (GGUF quantizations)
#   - Benchmark settings
#
# See training_params.yaml for all training configuration.
# Use quick_test.yaml for rapid testing with minimal resources.
#
# Run training:
#   python scripts/train.py
#   python scripts/train.py --config quick_test.yaml
#   python scripts/train.py --config my_custom_config.yaml
# =============================================================================
